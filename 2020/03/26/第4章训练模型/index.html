<!DOCTYPE html>
<html lang="zh_Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="KissingIce" />



<meta name="description" content="参考：作者的Jupyter NotebookChapter 4 – Training Linear Models  生成图片并保存 123456789101112131415161718192021from __future__ import division, print_function, unicode_literalsimport numpy as npimport matplotlib">
<meta property="og:type" content="article">
<meta property="og:title" content="第4章训练模型">
<meta property="og:url" content="http://yoursite.com/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="KissingIce">
<meta property="og:description" content="参考：作者的Jupyter NotebookChapter 4 – Training Linear Models  生成图片并保存 123456789101112131415161718192021from __future__ import division, print_function, unicode_literalsimport numpy as npimport matplotlib">
<meta property="article:published_time" content="2020-03-26T11:55:09.000Z">
<meta property="article:modified_time" content="2020-03-28T07:54:58.644Z">
<meta property="article:author" content="KissingIce">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="KissingIce" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>第4章训练模型 | KissingIce</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">KissingIce</a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="/3185849736@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c%E8%AF%AD%E8%A8%80/" rel="tag">c语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/" target="_blank" rel="noopener">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/" target="_blank" rel="noopener">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">KissingIce</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">KissingIce</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="/3185849736@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-第4章训练模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" class="article-date">
      <time datetime="2020-03-26T11:55:09.000Z" itemprop="datePublished">2020-03-26</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第4章训练模型
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        

        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考：作者的<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/tree/master/" target="_blank" rel="noopener">Jupyter Notebook</a><br><a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/04_training_linear_models.ipynb" target="_blank" rel="noopener">Chapter 4 – Training Linear Models</a></p>
<ol>
<li>生成图片并保存 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import division, print_function, unicode_literals</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import os</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">mpl.rc(&#39;axes&#39;, labelsize&#x3D;14)</span><br><span class="line">mpl.rc(&#39;xtick&#39;, labelsize&#x3D;12)</span><br><span class="line">mpl.rc(&#39;ytick&#39;, labelsize&#x3D;12)</span><br><span class="line"></span><br><span class="line"># Where to save the figures</span><br><span class="line">PROJECT_ROOT_DIR &#x3D; &quot;images&quot;</span><br><span class="line">CHAPTER_ID &#x3D; &quot;traininglinearmodels&quot;</span><br><span class="line"></span><br><span class="line">def save_fig(fig_id, tight_layout&#x3D;True):</span><br><span class="line">    path &#x3D; os.path.join(PROJECT_ROOT_DIR, CHAPTER_ID, fig_id + &quot;.png&quot;)</span><br><span class="line">    print(&quot;Saving figure&quot;, fig_id)</span><br><span class="line">    if tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format&#x3D;&#39;png&#39;, dpi&#x3D;600)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><ol start="2">
<li><p>生成一些线性数据来测试这个公式（标准方程）</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">X &#x3D; 2 * np.random.rand(100, 1)</span><br><span class="line">y &#x3D; 4 + 3 * X + np.random.randn(100, 1)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">#save_fig(&quot;generated_data_plot&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用NumPy的线性代数模块（np.linalg）中的inv（）函数来对矩阵求逆，并用dot（）方法计算矩阵的内积：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_b &#x3D; np.c_[np.ones((100, 1)), X]  # add x0 &#x3D; 1 to each instance</span><br><span class="line">theta_best &#x3D; np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line">#print(theta_best)</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以用*做出预测：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">X_new &#x3D; np.array([[0], [2]])</span><br><span class="line">X_new_b &#x3D; np.c_[np.ones((2, 1)), X_new]  # add x0 &#x3D; 1 to each instance</span><br><span class="line">y_predict &#x3D; X_new_b.dot(theta_best)</span><br><span class="line">#print(y_predict)</span><br><span class="line"></span><br><span class="line">#绘制模型的预测结果</span><br><span class="line">plt.plot(X_new, y_predict, &quot;r-&quot;)</span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">plt.plot(X_new, y_predict, &quot;r-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Predictions&quot;)</span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">#save_fig(&quot;linear_model_predictions&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Scikit-Learn的等效代码如下所示</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line">print(lin_reg.intercept_, lin_reg.coef_)</span><br><span class="line">print(lin_reg.predict(X_new))</span><br><span class="line">theta_best_svd, residuals, rank, s &#x3D; np.linalg.lstsq(X_b, y, rcond&#x3D;1e-6)</span><br><span class="line">print(theta_best_svd)</span><br><span class="line">print(np.linalg.pinv(X_b).dot(y))</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间如果学习率太高，那你可能会越过山谷直接到达山的另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案</p>
<ol start="6">
<li><p>批量梯度下降:3个公式，这个算法的快速实现：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eta &#x3D; 0.1</span><br><span class="line">n_iterations &#x3D; 1000</span><br><span class="line">m &#x3D; 100</span><br><span class="line">theta &#x3D; np.random.randn(2,1)</span><br><span class="line"></span><br><span class="line">for iteration in range(n_iterations):</span><br><span class="line">    gradients &#x3D; 2&#x2F;m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">    theta &#x3D; theta - eta * gradients</span><br><span class="line">#print(theta)</span><br><span class="line">#print(X_new_b.dot(theta))</span><br></pre></td></tr></table></figure>
</li>
<li><p>分别使用三种不同的学习率时，梯度下降的前十步:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">theta_path_bgd &#x3D; []</span><br><span class="line">def plot_gradient_descent(theta, eta, theta_path&#x3D;None):</span><br><span class="line">    m &#x3D; len(X_b)</span><br><span class="line">    plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">    n_iterations &#x3D; 1000</span><br><span class="line">    for iteration in range(n_iterations):</span><br><span class="line">        if iteration &lt; 10:</span><br><span class="line">            y_predict &#x3D; X_new_b.dot(theta)</span><br><span class="line">            style &#x3D; &quot;b-&quot; if iteration &gt; 0 else &quot;r--&quot;</span><br><span class="line">            plt.plot(X_new, y_predict, style)</span><br><span class="line">        gradients &#x3D; 2&#x2F;m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        if theta_path is not None:</span><br><span class="line">            theta_path.append(theta)</span><br><span class="line">    plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">    plt.axis([0, 2, 0, 15])</span><br><span class="line">    plt.title(r&quot;$\eta &#x3D; &#123;&#125;$&quot;.format(eta), fontsize&#x3D;16)</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line">theta &#x3D; np.random.randn(2,1)  # random initialization</span><br><span class="line">plt.figure(figsize&#x3D;(10,4))</span><br><span class="line">plt.subplot(131); plot_gradient_descent(theta, eta&#x3D;0.02)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.subplot(132); plot_gradient_descent(theta, eta&#x3D;0.1, theta_path&#x3D;theta_path_bgd)</span><br><span class="line">plt.subplot(133); plot_gradient_descent(theta, eta&#x3D;0.5)</span><br><span class="line">#save_fig(&quot;gradient_descent_plot&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面这段代码使用了一个简单的学习计划实现随机梯度下降：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">theta_path_sgd &#x3D; []</span><br><span class="line">m &#x3D; len(X_b)</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">n_epochs &#x3D; 50</span><br><span class="line">t0, t1 &#x3D; 5, 50  # learning schedule hyperparameters</span><br><span class="line"></span><br><span class="line">def learning_schedule(t):</span><br><span class="line">    return t0 &#x2F; (t + t1)</span><br><span class="line"></span><br><span class="line">theta &#x3D; np.random.randn(2,1)  # random initialization</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    for i in range(m):</span><br><span class="line">        if epoch &#x3D;&#x3D; 0 and i &lt; 20:                    # not shown in the book</span><br><span class="line">            y_predict &#x3D; X_new_b.dot(theta)           # not shown</span><br><span class="line">            style &#x3D; &quot;b-&quot; if i &gt; 0 else &quot;r--&quot;         # not shown</span><br><span class="line">            plt.plot(X_new, y_predict, style)        # not shown</span><br><span class="line">        random_index &#x3D; np.random.randint(m)</span><br><span class="line">        xi &#x3D; X_b[random_index:random_index+1]</span><br><span class="line">        yi &#x3D; y[random_index:random_index+1]</span><br><span class="line">        gradients &#x3D; 2 * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        eta &#x3D; learning_schedule(epoch * m + i)</span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        theta_path_sgd.append(theta)                 # not shown</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)                                 # not shown</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)                     # not shown</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)           # not shown</span><br><span class="line">plt.axis([0, 2, 0, 15])                              # not shown</span><br><span class="line">#save_fig(&quot;sgd_plot&quot;)                                 # not shown</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line">from sklearn.linear_model import SGDRegressor</span><br><span class="line">sgd_reg &#x3D; SGDRegressor(n_iter&#x3D;50, penalty&#x3D;None, eta0&#x3D;0.1)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">print(sgd_reg.intercept_, sgd_reg.coef_)</span><br></pre></td></tr></table></figure>
</li>
<li><p>小批量梯度下降</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">theta_path_bgd &#x3D; []</span><br><span class="line">theta_path_sgd &#x3D; []</span><br><span class="line">theta_path_mgd &#x3D; []</span><br><span class="line">m &#x3D; 100</span><br><span class="line">n_iterations &#x3D; 50</span><br><span class="line">minibatch_size &#x3D; 20</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line">theta &#x3D; np.random.randn(2,1)  # random initialization</span><br><span class="line"></span><br><span class="line">t0, t1 &#x3D; 200, 1000</span><br><span class="line">def learning_schedule(t):</span><br><span class="line">    return t0 &#x2F; (t + t1)</span><br><span class="line"></span><br><span class="line">t &#x3D; 0</span><br><span class="line">for epoch in range(n_iterations):</span><br><span class="line">    shuffled_indices &#x3D; np.random.permutation(m)</span><br><span class="line">    X_b_shuffled &#x3D; X_b[shuffled_indices]</span><br><span class="line">    y_shuffled &#x3D; y[shuffled_indices]</span><br><span class="line">    for i in range(0, m, minibatch_size):</span><br><span class="line">        t +&#x3D; 1</span><br><span class="line">        xi &#x3D; X_b_shuffled[i:i+minibatch_size]</span><br><span class="line">        yi &#x3D; y_shuffled[i:i+minibatch_size]</span><br><span class="line">        gradients &#x3D; 2&#x2F;minibatch_size * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        eta &#x3D; learning_schedule(t)</span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        theta_path_mgd.append(theta)</span><br><span class="line"></span><br><span class="line">theta_path_bgd &#x3D; np.array(theta_path_bgd)</span><br><span class="line">theta_path_sgd &#x3D; np.array(theta_path_sgd)</span><br><span class="line">theta_path_mgd &#x3D; np.array(theta_path_mgd)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(7,4))</span><br><span class="line">plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], &quot;r-s&quot;, linewidth&#x3D;1, label&#x3D;&quot;Stochastic&quot;)</span><br><span class="line">plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], &quot;g-+&quot;, linewidth&#x3D;2, label&#x3D;&quot;Mini-batch&quot;)</span><br><span class="line">plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], &quot;b-o&quot;, linewidth&#x3D;3, label&#x3D;&quot;Batch&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;16)</span><br><span class="line">plt.xlabel(r&quot;$\theta_0$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.ylabel(r&quot;$\theta_1$   &quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line">plt.axis([2.5, 4.5, 2.3, 3.9])</span><br><span class="line">save_fig(&quot;gradient_descent_paths_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><ol start="10">
<li><p>基于简单的二次方程（注：二次方程的形式为y=ax2+bx+c）制造一些非线性数据（添加随机噪声)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import numpy.random as rnd</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">m &#x3D; 100</span><br><span class="line">X &#x3D; 6 * np.random.rand(m, 1) - 3</span><br><span class="line">y &#x3D; 0.5 * X**2 + X + 2 + np.random.randn(m, 1)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.axis([-3, 3, 0, 10])</span><br><span class="line">save_fig(&quot;quadratic_data_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Scikit-Learn的PolynomialFeatures类来对训练数据进行转换</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">poly_features &#x3D; PolynomialFeatures(degree&#x3D;2, include_bias&#x3D;False)</span><br><span class="line">X_poly &#x3D; poly_features.fit_transform(X)</span><br><span class="line">#print(X[0])</span><br><span class="line">#print(X_poly[0])</span><br></pre></td></tr></table></figure>
</li>
<li><p>对这个扩展后的训练集匹配一个LinearRegression模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br><span class="line">#print(lin_reg.intercept_, lin_reg.coef_)</span><br><span class="line"></span><br><span class="line">X_new&#x3D;np.linspace(-3, 3, 100).reshape(100, 1)</span><br><span class="line">X_new_poly &#x3D; poly_features.transform(X_new)</span><br><span class="line">y_new &#x3D; lin_reg.predict(X_new_poly)</span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.plot(X_new, y_new, &quot;r-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Predictions&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([-3, 3, 0, 10])</span><br><span class="line">#save_fig(&quot;quadratic_predictions_plot(多项式回归模型预测)&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><ol start="13">
<li><p>高阶多项式回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">for style, width, degree in ((&quot;g-&quot;, 1, 300), (&quot;b--&quot;, 2, 2), (&quot;r-+&quot;, 2, 1)):</span><br><span class="line">    polybig_features &#x3D; PolynomialFeatures(degree&#x3D;degree, include_bias&#x3D;False)</span><br><span class="line">    std_scaler &#x3D; StandardScaler()</span><br><span class="line">    lin_reg &#x3D; LinearRegression()</span><br><span class="line">    polynomial_regression &#x3D; Pipeline([</span><br><span class="line">            (&quot;poly_features&quot;, polybig_features),</span><br><span class="line">            (&quot;std_scaler&quot;, std_scaler),</span><br><span class="line">            (&quot;lin_reg&quot;, lin_reg),</span><br><span class="line">        ])</span><br><span class="line">    polynomial_regression.fit(X, y)</span><br><span class="line">    y_newbig &#x3D; polynomial_regression.predict(X_new)</span><br><span class="line">    plt.plot(X_new, y_newbig, style, label&#x3D;str(degree), linewidth&#x3D;width)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;, linewidth&#x3D;3)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.axis([-3, 3, 0, 10])</span><br><span class="line">#save_fig(&quot;high_degree_polynomials_plot(高阶多项式回归)&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>纯线性回归模型学习曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">def plot_learning_curves(model, X, y):</span><br><span class="line">    X_train, X_val, y_train, y_val &#x3D; train_test_split(X, y, test_size&#x3D;0.2, random_state&#x3D;10)</span><br><span class="line">    train_errors, val_errors &#x3D; [], []</span><br><span class="line">    for m in range(1, len(X_train)):</span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        y_train_predict &#x3D; model.predict(X_train[:m])</span><br><span class="line">        y_val_predict &#x3D; model.predict(X_val)</span><br><span class="line">        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">    plt.plot(np.sqrt(train_errors), &quot;r-+&quot;, linewidth&#x3D;2, label&#x3D;&quot;train&quot;)</span><br><span class="line">    plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth&#x3D;3, label&#x3D;&quot;val&quot;)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;upper right&quot;, fontsize&#x3D;14)   # not shown in the book</span><br><span class="line">    plt.xlabel(&quot;Training set size&quot;, fontsize&#x3D;14) # not shown</span><br><span class="line">    plt.ylabel(&quot;RMSE&quot;, fontsize&#x3D;14)              # not shown</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br><span class="line">plt.axis([0, 80, 0, 3])                         # not shown in the book</span><br><span class="line">#save_fig(&quot;underfitting_learning_curves_plot(学习曲线)&quot;)   # not shown</span><br><span class="line">#plt.show()                                      # not shown</span><br></pre></td></tr></table></figure>
</li>
<li><p>多项式回归模型的学习曲线 10阶</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression &#x3D; Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;10, include_bias&#x3D;False)),</span><br><span class="line">        (&quot;lin_reg&quot;, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([0, 80, 0, 3])           # not shown</span><br><span class="line">save_fig(&quot;learning_curves_plot(多项式回归模型的学习曲线)&quot;)  # not shown</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="正则线性模型"><a href="#正则线性模型" class="headerlink" title="正则线性模型"></a>正则线性模型</h4><ol start="16">
<li><p>岭回归（也叫作吉洪诺夫正则化）是线性回归的正则化版</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 20</span><br><span class="line">X &#x3D; 3 * np.random.rand(m, 1)</span><br><span class="line">y &#x3D; 1 + 0.5 * X + np.random.randn(m, 1) &#x2F; 1.5</span><br><span class="line">X_new &#x3D; np.linspace(0, 3, 100).reshape(100, 1)</span><br><span class="line"></span><br><span class="line">def plot_model(model_class, polynomial, alphas, **model_kargs):</span><br><span class="line">    for alpha, style in zip(alphas, (&quot;b-&quot;, &quot;g--&quot;, &quot;r:&quot;)):</span><br><span class="line">        model &#x3D; model_class(alpha, **model_kargs) if alpha &gt; 0 else LinearRegression()</span><br><span class="line">        if polynomial:</span><br><span class="line">            model &#x3D; Pipeline([</span><br><span class="line">                    (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;10, include_bias&#x3D;False)),</span><br><span class="line">                    (&quot;std_scaler&quot;, StandardScaler()),</span><br><span class="line">                    (&quot;regul_reg&quot;, model),</span><br><span class="line">                ])</span><br><span class="line">        model.fit(X, y)</span><br><span class="line">        y_new_regul &#x3D; model.predict(X_new)</span><br><span class="line">        lw &#x3D; 2 if alpha &gt; 0 else 1</span><br><span class="line">        plt.plot(X_new, y_new_regul, style, linewidth&#x3D;lw, label&#x3D;r&quot;$\alpha &#x3D; &#123;&#125;$&quot;.format(alpha))</span><br><span class="line">    plt.plot(X, y, &quot;b.&quot;, linewidth&#x3D;3)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;15)</span><br><span class="line">    plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">    plt.axis([0, 3, 0, 4])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8,4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_model(Ridge, polynomial&#x3D;False, alphas&#x3D;(0, 10, 100), random_state&#x3D;42)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_model(Ridge, polynomial&#x3D;True, alphas&#x3D;(0, 10**-5, 1), random_state&#x3D;42)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;ridge_regression_plot(岭回归)&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Scikit-Learn执行闭式解的岭回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line">ridge_reg &#x3D; Ridge(alpha&#x3D;1, solver&#x3D;&quot;cholesky&quot;, random_state&#x3D;42)</span><br><span class="line">ridge_reg.fit(X, y)</span><br><span class="line">ridge_reg.predict([[1.5]])</span><br><span class="line"></span><br><span class="line">ridge_reg &#x3D; Ridge(alpha&#x3D;1, solver&#x3D;&quot;sag&quot;, random_state&#x3D;42)</span><br><span class="line">ridge_reg.fit(X, y)</span><br><span class="line">ridge_reg.predict([[1.5]])</span><br><span class="line"></span><br><span class="line">#使用随机梯度下降</span><br><span class="line">from sklearn.linear_model import SGDRegressor</span><br><span class="line">sgd_reg &#x3D; SGDRegressor(max_iter&#x3D;50, tol&#x3D;-np.infty, penalty&#x3D;&quot;l2&quot;, random_state&#x3D;42)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.predict([[1.5]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>套索回归、Lasso回归.线性回归的另一种正则化，叫作最小绝对收缩和选择算子回归（Least Absolute Shrinkage and Selection Operator Regression，简称Lasso回归，或套索回归）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8,4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_model(Lasso, polynomial&#x3D;False, alphas&#x3D;(0, 0.1, 1), random_state&#x3D;42)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_model(Lasso, polynomial&#x3D;True, alphas&#x3D;(0, 10**-7, 1), tol&#x3D;1, random_state&#x3D;42)</span><br><span class="line">#save_fig(&quot;lasso_regression_plot(套索回归Lasso回归)&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Scikit-Learn的Lasso类的小例子。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import ElasticNet</span><br><span class="line">elastic_net &#x3D; ElasticNet(alpha&#x3D;0.1, l1_ratio&#x3D;0.5, random_state&#x3D;42)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line">#print(elastic_net.predict([[1.5]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>弹性网络，使用Scikit-Learn的ElasticNet的小例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import ElasticNet</span><br><span class="line">elastic_net &#x3D; ElasticNet(alpha&#x3D;0.1, l1_ratio&#x3D;0.5, random_state&#x3D;42)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line">#print(elastic_net.predict([[1.5]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>早期停止法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import SGDRegressor</span><br><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 100</span><br><span class="line">X &#x3D; 6 * np.random.rand(m, 1) - 3</span><br><span class="line">y &#x3D; 2 + X + 0.5 * X**2 + np.random.randn(m, 1)</span><br><span class="line"></span><br><span class="line">X_train, X_val, y_train, y_val &#x3D; train_test_split(X[:50], y[:50].ravel(), test_size&#x3D;0.5, random_state&#x3D;10)</span><br><span class="line"></span><br><span class="line">poly_scaler &#x3D; Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;90, include_bias&#x3D;False)),</span><br><span class="line">        (&quot;std_scaler&quot;, StandardScaler()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">X_train_poly_scaled &#x3D; poly_scaler.fit_transform(X_train)</span><br><span class="line">X_val_poly_scaled &#x3D; poly_scaler.transform(X_val)</span><br><span class="line"></span><br><span class="line">sgd_reg &#x3D; SGDRegressor(max_iter&#x3D;1,</span><br><span class="line">                    tol&#x3D;-np.infty,</span><br><span class="line">                    penalty&#x3D;None,</span><br><span class="line">                    eta0&#x3D;0.0005,</span><br><span class="line">                    warm_start&#x3D;True,</span><br><span class="line">                    learning_rate&#x3D;&quot;constant&quot;,</span><br><span class="line">                    random_state&#x3D;42)</span><br><span class="line"></span><br><span class="line">n_epochs &#x3D; 500</span><br><span class="line">train_errors, val_errors &#x3D; [], []</span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    sgd_reg.fit(X_train_poly_scaled, y_train)</span><br><span class="line">    y_train_predict &#x3D; sgd_reg.predict(X_train_poly_scaled)</span><br><span class="line">    y_val_predict &#x3D; sgd_reg.predict(X_val_poly_scaled)</span><br><span class="line">    train_errors.append(mean_squared_error(y_train, y_train_predict))</span><br><span class="line">    val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">best_epoch &#x3D; np.argmin(val_errors)</span><br><span class="line">best_val_rmse &#x3D; np.sqrt(val_errors[best_epoch])</span><br><span class="line"></span><br><span class="line">plt.annotate(&#39;Best model&#39;,</span><br><span class="line">            xy&#x3D;(best_epoch, best_val_rmse),</span><br><span class="line">            xytext&#x3D;(best_epoch, best_val_rmse + 1),</span><br><span class="line">            ha&#x3D;&quot;center&quot;,</span><br><span class="line">            arrowprops&#x3D;dict(facecolor&#x3D;&#39;black&#39;, shrink&#x3D;0.05),</span><br><span class="line">            fontsize&#x3D;16,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">best_val_rmse -&#x3D; 0.03  # just to make the graph look better</span><br><span class="line">plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], &quot;k:&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth&#x3D;3, label&#x3D;&quot;Validation set&quot;)</span><br><span class="line">plt.plot(np.sqrt(train_errors), &quot;r--&quot;, linewidth&#x3D;2, label&#x3D;&quot;Training set&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper right&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.xlabel(&quot;Epoch&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;RMSE&quot;, fontsize&#x3D;14)</span><br><span class="line">save_fig(&quot;early_stopping_plot(早期停止法)&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Lasso回归与岭回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">t1a, t1b, t2a, t2b &#x3D; -1, 3, -1.5, 1.5</span><br><span class="line"></span><br><span class="line"># ignoring bias term</span><br><span class="line">t1s &#x3D; np.linspace(t1a, t1b, 500)</span><br><span class="line">t2s &#x3D; np.linspace(t2a, t2b, 500)</span><br><span class="line">t1, t2 &#x3D; np.meshgrid(t1s, t2s)</span><br><span class="line">T &#x3D; np.c_[t1.ravel(), t2.ravel()]</span><br><span class="line">Xr &#x3D; np.array([[-1, 1], [-0.3, -1], [1, 0.1]])</span><br><span class="line">yr &#x3D; 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]</span><br><span class="line"></span><br><span class="line">J &#x3D; (1&#x2F;len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis&#x3D;1)).reshape(t1.shape)</span><br><span class="line"></span><br><span class="line">N1 &#x3D; np.linalg.norm(T, ord&#x3D;1, axis&#x3D;1).reshape(t1.shape)</span><br><span class="line">N2 &#x3D; np.linalg.norm(T, ord&#x3D;2, axis&#x3D;1).reshape(t1.shape)</span><br><span class="line"></span><br><span class="line">t_min_idx &#x3D; np.unravel_index(np.argmin(J), J.shape)</span><br><span class="line">t1_min, t2_min &#x3D; t1[t_min_idx], t2[t_min_idx]</span><br><span class="line"></span><br><span class="line">t_init &#x3D; np.array([[0.25], [-1]])</span><br><span class="line"></span><br><span class="line">def bgd_path(theta, X, y, l1, l2, core &#x3D; 1, eta &#x3D; 0.1, n_iterations &#x3D; 50):</span><br><span class="line">    path &#x3D; [theta]</span><br><span class="line">    for iteration in range(n_iterations):</span><br><span class="line">        gradients &#x3D; core * 2&#x2F;len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + 2 * l2 * theta</span><br><span class="line"></span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        path.append(theta)</span><br><span class="line">    return np.array(path)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(12, 8))</span><br><span class="line">for i, N, l1, l2, title in ((0, N1, 0.5, 0, &quot;Lasso&quot;), (1, N2, 0,  0.1, &quot;Ridge&quot;)):</span><br><span class="line">    JR &#x3D; J + l1 * N1 + l2 * N2**2</span><br><span class="line">    </span><br><span class="line">    tr_min_idx &#x3D; np.unravel_index(np.argmin(JR), JR.shape)</span><br><span class="line">    t1r_min, t2r_min &#x3D; t1[tr_min_idx], t2[tr_min_idx]</span><br><span class="line"></span><br><span class="line">    levelsJ&#x3D;(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)</span><br><span class="line">    levelsJR&#x3D;(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)</span><br><span class="line">    levelsN&#x3D;np.linspace(0, np.max(N), 10)</span><br><span class="line">    </span><br><span class="line">    path_J &#x3D; bgd_path(t_init, Xr, yr, l1&#x3D;0, l2&#x3D;0)</span><br><span class="line">    path_JR &#x3D; bgd_path(t_init, Xr, yr, l1, l2)</span><br><span class="line">    path_N &#x3D; bgd_path(t_init, Xr, yr, np.sign(l1)&#x2F;3, np.sign(l2), core&#x3D;0)</span><br><span class="line"></span><br><span class="line">    plt.subplot(221 + i * 2)</span><br><span class="line">    plt.grid(True)</span><br><span class="line">    plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.contourf(t1, t2, J, levels&#x3D;levelsJ, alpha&#x3D;0.9)</span><br><span class="line">    plt.contour(t1, t2, N, levels&#x3D;levelsN)</span><br><span class="line">    plt.plot(path_J[:, 0], path_J[:, 1], &quot;w-o&quot;)</span><br><span class="line">    plt.plot(path_N[:, 0], path_N[:, 1], &quot;y-^&quot;)</span><br><span class="line">    plt.plot(t1_min, t2_min, &quot;rs&quot;)</span><br><span class="line">    plt.title(r&quot;$\ell_&#123;&#125;$ penalty&quot;.format(i + 1), fontsize&#x3D;16)</span><br><span class="line">    plt.axis([t1a, t1b, t2a, t2b])</span><br><span class="line">    if i &#x3D;&#x3D; 1:</span><br><span class="line">        plt.xlabel(r&quot;$\theta_1$&quot;, fontsize&#x3D;20)</span><br><span class="line">    plt.ylabel(r&quot;$\theta_2$&quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line"></span><br><span class="line">    plt.subplot(222 + i * 2)</span><br><span class="line">    plt.grid(True)</span><br><span class="line">    plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.contourf(t1, t2, JR, levels&#x3D;levelsJR, alpha&#x3D;0.9)</span><br><span class="line">    plt.plot(path_JR[:, 0], path_JR[:, 1], &quot;w-o&quot;)</span><br><span class="line">    plt.plot(t1r_min, t2r_min, &quot;rs&quot;)</span><br><span class="line">    plt.title(title, fontsize&#x3D;16)</span><br><span class="line">    plt.axis([t1a, t1b, t2a, t2b])</span><br><span class="line">    if i &#x3D;&#x3D; 1:</span><br><span class="line">        plt.xlabel(r&quot;$\theta_1$&quot;, fontsize&#x3D;20)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;lasso_vs_ridge_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>逻辑回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#逻辑函数</span><br><span class="line">t &#x3D; np.linspace(-10, 10, 100)</span><br><span class="line">sig &#x3D; 1 &#x2F; (1 + np.exp(-t))</span><br><span class="line">plt.figure(figsize&#x3D;(9, 3))</span><br><span class="line">plt.plot([-10, 10], [0, 0], &quot;k-&quot;)</span><br><span class="line">plt.plot([-10, 10], [0.5, 0.5], &quot;k:&quot;)</span><br><span class="line">plt.plot([-10, 10], [1, 1], &quot;k:&quot;)</span><br><span class="line">plt.plot([0, 0], [-1.1, 1.1], &quot;k-&quot;)</span><br><span class="line">plt.plot(t, sig, &quot;b-&quot;, linewidth&#x3D;2, label&#x3D;r&quot;$\sigma(t) &#x3D; \frac&#123;1&#125;&#123;1 + e^&#123;-t&#125;&#125;$&quot;)</span><br><span class="line">plt.xlabel(&quot;t&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.axis([-10, 10, -0.1, 1.1])</span><br><span class="line">save_fig(&quot;logistic_function_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>决策边界</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#创建一个分类器来检测Virginica鸢尾花。</span><br><span class="line">from sklearn import datasets</span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">list(iris.keys())</span><br><span class="line">#print(list(iris.keys()))</span><br><span class="line">#print(iris.DESCR)</span><br><span class="line"></span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, 3:]  # petal width</span><br><span class="line">y &#x3D; (iris[&quot;target&quot;] &#x3D;&#x3D; 2).astype(np.int)  # 1 if Iris-Virginica, else 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练逻辑回归模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">log_reg &#x3D; LogisticRegression(solver&#x3D;&quot;liblinear&quot;, random_state&#x3D;42)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">#精简版</span><br><span class="line">X_new &#x3D; np.linspace(0, 3, 1000).reshape(-1, 1)</span><br><span class="line">y_proba &#x3D; log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth&#x3D;2, label&#x3D;&quot;Not Iris-Virginica&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">#完整版</span><br><span class="line">X_new &#x3D; np.linspace(0, 3, 1000).reshape(-1, 1)</span><br><span class="line">y_proba &#x3D; log_reg.predict_proba(X_new)</span><br><span class="line">decision_boundary &#x3D; X_new[y_proba[:, 1] &gt;&#x3D; 0.5][0]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8, 3))</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;0], y[y&#x3D;&#x3D;0], &quot;bs&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;1], y[y&#x3D;&#x3D;1], &quot;g^&quot;)</span><br><span class="line">plt.plot([decision_boundary, decision_boundary], [-1, 2], &quot;k:&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth&#x3D;2, label&#x3D;&quot;Not Iris-Virginica&quot;)</span><br><span class="line">plt.text(decision_boundary+0.02, 0.15, &quot;Decision  boundary&quot;, fontsize&#x3D;14, color&#x3D;&quot;k&quot;, ha&#x3D;&quot;center&quot;)</span><br><span class="line">plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width&#x3D;0.05, head_length&#x3D;0.1, fc&#x3D;&#39;b&#39;, ec&#x3D;&#39;b&#39;)</span><br><span class="line">plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width&#x3D;0.05, head_length&#x3D;0.1, fc&#x3D;&#39;g&#39;, ec&#x3D;&#39;g&#39;)</span><br><span class="line">plt.xlabel(&quot;Petal width (cm)&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Probability&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.legend(loc&#x3D;&quot;center left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 3, -0.02, 1.02])</span><br><span class="line">#save_fig(&quot;logistic_regression_plot(估算概率和决策边界)&quot;)</span><br><span class="line">#plt.show()</span><br><span class="line">print(decision_boundary)</span><br><span class="line">print(log_reg.predict([[1.7], [1.5]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Softmax回归 多元逻辑回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y &#x3D; (iris[&quot;target&quot;] &#x3D;&#x3D; 2).astype(np.int)</span><br><span class="line"></span><br><span class="line">log_reg &#x3D; LogisticRegression(solver&#x3D;&quot;liblinear&quot;, C&#x3D;10**10, random_state&#x3D;42)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">x0, x1 &#x3D; np.meshgrid(</span><br><span class="line">        np.linspace(2.9, 7, 500).reshape(-1, 1),</span><br><span class="line">        np.linspace(0.8, 2.7, 200).reshape(-1, 1),</span><br><span class="line">    )</span><br><span class="line">X_new &#x3D; np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line">y_proba &#x3D; log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(10, 4))</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;0, 0], X[y&#x3D;&#x3D;0, 1], &quot;bs&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;1, 0], X[y&#x3D;&#x3D;1, 1], &quot;g^&quot;)</span><br><span class="line"></span><br><span class="line">zz &#x3D; y_proba[:, 1].reshape(x0.shape)</span><br><span class="line">contour &#x3D; plt.contour(x0, x1, zz, cmap&#x3D;plt.cm.brg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">left_right &#x3D; np.array([2.9, 7])</span><br><span class="line">boundary &#x3D; -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) &#x2F; log_reg.coef_[0][1]</span><br><span class="line"></span><br><span class="line">plt.clabel(contour, inline&#x3D;1, fontsize&#x3D;12)</span><br><span class="line">plt.plot(left_right, boundary, &quot;k--&quot;, linewidth&#x3D;3)</span><br><span class="line">plt.text(3.5, 1.5, &quot;Not Iris-Virginica&quot;, fontsize&#x3D;14, color&#x3D;&quot;b&quot;, ha&#x3D;&quot;center&quot;)</span><br><span class="line">plt.text(6.5, 2.3, &quot;Iris-Virginica&quot;, fontsize&#x3D;14, color&#x3D;&quot;g&quot;, ha&#x3D;&quot;center&quot;)</span><br><span class="line">plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Petal width&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([2.9, 7, 0.8, 2.7])</span><br><span class="line">save_fig(&quot;logistic_regression_contour_plot&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y &#x3D; iris[&quot;target&quot;]</span><br><span class="line"></span><br><span class="line">softmax_reg &#x3D; LogisticRegression(multi_class&#x3D;&quot;multinomial&quot;,solver&#x3D;&quot;lbfgs&quot;, C&#x3D;10, random_state&#x3D;42)</span><br><span class="line">softmax_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">x0, x1 &#x3D; np.meshgrid(</span><br><span class="line">        np.linspace(0, 8, 500).reshape(-1, 1),</span><br><span class="line">        np.linspace(0, 3.5, 200).reshape(-1, 1),</span><br><span class="line">    )</span><br><span class="line">X_new &#x3D; np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line">y_proba &#x3D; softmax_reg.predict_proba(X_new)</span><br><span class="line">y_predict &#x3D; softmax_reg.predict(X_new)</span><br><span class="line"></span><br><span class="line">zz1 &#x3D; y_proba[:, 1].reshape(x0.shape)</span><br><span class="line">zz &#x3D; y_predict.reshape(x0.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(10, 4))</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;2, 0], X[y&#x3D;&#x3D;2, 1], &quot;g^&quot;, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;1, 0], X[y&#x3D;&#x3D;1, 1], &quot;bs&quot;, label&#x3D;&quot;Iris-Versicolor&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;0, 0], X[y&#x3D;&#x3D;0, 1], &quot;yo&quot;, label&#x3D;&quot;Iris-Setosa&quot;)</span><br><span class="line"></span><br><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line">custom_cmap &#x3D; ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;])</span><br><span class="line"></span><br><span class="line">plt.contourf(x0, x1, zz, cmap&#x3D;custom_cmap)</span><br><span class="line">contour &#x3D; plt.contour(x0, x1, zz1, cmap&#x3D;plt.cm.brg)</span><br><span class="line">plt.clabel(contour, inline&#x3D;1, fontsize&#x3D;12)</span><br><span class="line">plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Petal width&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.legend(loc&#x3D;&quot;center left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 7, 0, 3.5])</span><br><span class="line">save_fig(&quot;softmax_regression_contour_plot&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(softmax_reg.predict([[5, 2]]))</span><br><span class="line">print(softmax_reg.predict_proba([[5, 2]]))</span><br></pre></td></tr></table></figure>



















</li>
</ol>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">第4章训练模型</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">KissingIce</a></p>
        <p><span>发布时间:</span>2020-03-26, 19:55:09</p>
        <p><span>最后更新:</span>2020-03-28, 15:54:58</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="第4章训练模型">http://yoursite.com/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</a>
            <span class="copy-path" data-clipboard-text="原文: http://yoursite.com/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/　　作者: KissingIce" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2020/03/28/%E7%AC%AC5%E7%AB%A0%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">
                    第5章支持向量机
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2020/03/24/python%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/">
                    python爬取网页图片
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#线性回归"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降"><span class="toc-number">2.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多项式回归"><span class="toc-number">3.</span> <span class="toc-text">多项式回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#学习曲线"><span class="toc-number">4.</span> <span class="toc-text">学习曲线</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#正则线性模型"><span class="toc-number">5.</span> <span class="toc-text">正则线性模型</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"第4章训练模型　| KissingIce　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2020/03/28/%E7%AC%AC5%E7%AB%A0%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="上一篇: 第5章支持向量机">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2020/03/24/python%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/" title="下一篇: python爬取网页图片">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/03/28/%E7%AC%AC5%E7%AB%A0%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">第5章支持向量机</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">第4章训练模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/24/python%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/">python爬取网页图片</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/18/%E7%AC%AC3%E7%AB%A0%E5%88%86%E7%B1%BB/">第3章分类</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/11/%E7%AC%AC2%E7%AB%A0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/">第2章端到端的机器学习项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/06/c%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AF%95%E5%87%86%E5%A4%87/">c语言笔试准备</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/05/c%E8%AF%AD%E8%A8%80%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/">c语言面试准备</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/03/hello-world/">Hello World</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2020 KissingIce
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 6;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>