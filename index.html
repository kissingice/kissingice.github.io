<!DOCTYPE html>
<html lang="zh_Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="KissingIce" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="KissingIce">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="KissingIce">
<meta property="article:author" content="KissingIce">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="KissingIce" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">



    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>KissingIce</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">KissingIce</a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="/3185849736@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c%E8%AF%AD%E8%A8%80/" rel="tag">c语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/" target="_blank" rel="noopener">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/" target="_blank" rel="noopener">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">KissingIce</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">KissingIce</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="/3185849736@qq.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-第6章决策树" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/29/%E7%AC%AC6%E7%AB%A0%E5%86%B3%E7%AD%96%E6%A0%91/" class="article-date">
      <time datetime="2020-03-29T07:38:14.000Z" itemprop="datePublished">2020-03-29</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/29/%E7%AC%AC6%E7%AB%A0%E5%86%B3%E7%AD%96%E6%A0%91/">第6章决策树</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考：作者的<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/tree/master/" target="_blank" rel="noopener">Jupyter Notebook</a><br><a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/06_decision_trees.ipynb" target="_blank" rel="noopener">Chapter 6 – Decision Trees</a></p>
<ol>
<li>保存图片 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import division, print_function, unicode_literals</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import os</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">mpl.rc(&#39;axes&#39;, labelsize&#x3D;14)</span><br><span class="line">mpl.rc(&#39;xtick&#39;, labelsize&#x3D;12)</span><br><span class="line">mpl.rc(&#39;ytick&#39;, labelsize&#x3D;12)</span><br><span class="line"></span><br><span class="line"># Where to save the figures</span><br><span class="line">PROJECT_ROOT_DIR &#x3D; &quot;images&quot;</span><br><span class="line">CHAPTER_ID &#x3D; &quot;decision_trees&quot;</span><br><span class="line"></span><br><span class="line">def save_fig(fig_id, tight_layout&#x3D;True):</span><br><span class="line">    path &#x3D; os.path.join(PROJECT_ROOT_DIR, CHAPTER_ID, fig_id + &quot;.png&quot;)</span><br><span class="line">    print(&quot;Saving figure&quot;, fig_id)</span><br><span class="line">    if tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format&#x3D;&#39;png&#39;, dpi&#x3D;600)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="决策树训练和可视化"><a href="#决策树训练和可视化" class="headerlink" title="决策树训练和可视化"></a>决策树训练和可视化</h4><ol start="2">
<li><p>要了解决策树，让我们先构建一个决策树，看看它是如何做出预测的。下面的代码在鸢尾花数据集（见第4章）上训练了一个DecisionTreeClassifier：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">iris &#x3D; load_iris()</span><br><span class="line">X &#x3D; iris.data[:, 2:] # petal length and width</span><br><span class="line">y &#x3D; iris.target</span><br><span class="line"></span><br><span class="line">tree_clf &#x3D; DecisionTreeClassifier(max_depth&#x3D;2, random_state&#x3D;42)</span><br><span class="line">tree_clf.fit(X, y)</span><br><span class="line">#print(tree_clf.fit(X, y))</span><br></pre></td></tr></table></figure>
</li>
<li><p>要将决策树可视化，首先，使用export_graphviz（）方法输出一个图形定义文件，命名为iris_tree.dot：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import export_graphviz</span><br><span class="line"></span><br><span class="line">export_graphviz(</span><br><span class="line">        tree_clf,</span><br><span class="line">        out_file&#x3D;image_path(&quot;iris_tree.dot&quot;),</span><br><span class="line">        feature_names&#x3D;iris.feature_names[2:],</span><br><span class="line">        class_names&#x3D;iris.target_names,</span><br><span class="line">        rounded&#x3D;True,</span><br><span class="line">        filled&#x3D;True</span><br><span class="line">    )</span><br><span class="line">#下面这行命令将.dot文件转换为.png图像文件：</span><br><span class="line">#$ dot -Tpng iris_tree.dot -o iris_tree.png</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="做出预测"><a href="#做出预测" class="headerlink" title="做出预测"></a>做出预测</h4><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line">def plot_decision_boundary(clf, X, y, axes&#x3D;[0, 7.5, 0, 3], iris&#x3D;True, legend&#x3D;False, plot_training&#x3D;True):</span><br><span class="line">    x1s &#x3D; np.linspace(axes[0], axes[1], 100)</span><br><span class="line">    x2s &#x3D; np.linspace(axes[2], axes[3], 100)</span><br><span class="line">    x1, x2 &#x3D; np.meshgrid(x1s, x2s)</span><br><span class="line">    X_new &#x3D; np.c_[x1.ravel(), x2.ravel()]</span><br><span class="line">    y_pred &#x3D; clf.predict(X_new).reshape(x1.shape)</span><br><span class="line">    custom_cmap &#x3D; ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;])</span><br><span class="line">    plt.contourf(x1, x2, y_pred, alpha&#x3D;0.3, cmap&#x3D;custom_cmap)</span><br><span class="line">    if not iris:</span><br><span class="line">        custom_cmap2 &#x3D; ListedColormap([&#39;#7d7d58&#39;,&#39;#4c4c7f&#39;,&#39;#507d50&#39;])</span><br><span class="line">        plt.contour(x1, x2, y_pred, cmap&#x3D;custom_cmap2, alpha&#x3D;0.8)</span><br><span class="line">    if plot_training:</span><br><span class="line">        plt.plot(X[:, 0][y&#x3D;&#x3D;0], X[:, 1][y&#x3D;&#x3D;0], &quot;yo&quot;, label&#x3D;&quot;Iris-Setosa&quot;)</span><br><span class="line">        plt.plot(X[:, 0][y&#x3D;&#x3D;1], X[:, 1][y&#x3D;&#x3D;1], &quot;bs&quot;, label&#x3D;&quot;Iris-Versicolor&quot;)</span><br><span class="line">        plt.plot(X[:, 0][y&#x3D;&#x3D;2], X[:, 1][y&#x3D;&#x3D;2], &quot;g^&quot;, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">        plt.axis(axes)</span><br><span class="line">    if iris:</span><br><span class="line">        plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">        plt.ylabel(&quot;Petal width&quot;, fontsize&#x3D;14)</span><br><span class="line">    else:</span><br><span class="line">        plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">        plt.ylabel(r&quot;$x_2$&quot;, fontsize&#x3D;18, rotation&#x3D;0)</span><br><span class="line">    if legend:</span><br><span class="line">        plt.legend(loc&#x3D;&quot;lower right&quot;, fontsize&#x3D;14)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8, 4))</span><br><span class="line">plot_decision_boundary(tree_clf, X, y)</span><br><span class="line">plt.plot([2.45, 2.45], [0, 3], &quot;k-&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot([2.45, 7.5], [1.75, 1.75], &quot;k--&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot([4.95, 4.95], [0, 1.75], &quot;k:&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot([4.85, 4.85], [1.75, 3], &quot;k:&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.text(1.40, 1.0, &quot;Depth&#x3D;0&quot;, fontsize&#x3D;15)</span><br><span class="line">plt.text(3.2, 1.80, &quot;Depth&#x3D;1&quot;, fontsize&#x3D;13)</span><br><span class="line">plt.text(4.05, 0.5, &quot;(Depth&#x3D;2)&quot;, fontsize&#x3D;11)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;decision_tree_decision_boundaries_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></code></pre><h4 id="估算类别概率"><a href="#估算类别概率" class="headerlink" title="估算类别概率"></a>估算类别概率</h4><ol start="4">
<li>决策树同样可以估算某个实例属于特定类别k的概率 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#print(tree_clf.predict_proba([[5, 1.5]]))</span><br><span class="line">#print(tree_clf.predict([[5, 1.5]]))0</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h4 id="CART训练算法"><a href="#CART训练算法" class="headerlink" title="CART训练算法"></a>CART训练算法</h4><p>Scikit-Learn使用的是分类与回归树（Classification And Regression Tree，简称CART）算法来训练决策树（也叫作“生长”树）。</p>
<h4 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h4><h4 id="基尼不纯度还是信息熵"><a href="#基尼不纯度还是信息熵" class="headerlink" title="基尼不纯度还是信息熵"></a>基尼不纯度还是信息熵</h4><h4 id="正则化超参数"><a href="#正则化超参数" class="headerlink" title="正则化超参数"></a>正则化超参数</h4><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_moons</span><br><span class="line">Xm, ym &#x3D; make_moons(n_samples&#x3D;100, noise&#x3D;0.25, random_state&#x3D;53)</span><br><span class="line"></span><br><span class="line">deep_tree_clf1 &#x3D; DecisionTreeClassifier(random_state&#x3D;42)</span><br><span class="line">deep_tree_clf2 &#x3D; DecisionTreeClassifier(min_samples_leaf&#x3D;4, random_state&#x3D;42)</span><br><span class="line">deep_tree_clf1.fit(Xm, ym)</span><br><span class="line">deep_tree_clf2.fit(Xm, ym)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(11, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_decision_boundary(deep_tree_clf1, Xm, ym, axes&#x3D;[-1.5, 2.5, -1, 1.5], iris&#x3D;False)</span><br><span class="line">plt.title(&quot;No restrictions&quot;, fontsize&#x3D;16)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_decision_boundary(deep_tree_clf2, Xm, ym, axes&#x3D;[-1.5, 2.5, -1, 1.5], iris&#x3D;False)</span><br><span class="line">plt.title(&quot;min_samples_leaf &#x3D; &#123;&#125;&quot;.format(deep_tree_clf2.min_samples_leaf), fontsize&#x3D;14)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;min_samples_leaf_plot正则化超参数&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></code></pre><p>左图使用默认参数（即无约束）来训练决策树，右图的决策树应用min_samples_leaf=4进行训练。很明显，左图模型过度拟合，右图的泛化效果更佳。</p>
<h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><ol start="5">
<li><p>决策树也可以执行回归任务。我们用Scikit_Learn的DecisionTreeRegressor来构建一个回归树，在一个带噪声的二次数据集上进行训练，其中max_depth=2：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 200</span><br><span class="line">X &#x3D; np.random.rand(m, 1)</span><br><span class="line">y &#x3D; 4 * (X - 0.5) ** 2</span><br><span class="line">y &#x3D; y + np.random.randn(m, 1) &#x2F; 10</span><br><span class="line"></span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">tree_reg &#x3D; DecisionTreeRegressor(max_depth&#x3D;2, random_state&#x3D;42)</span><br><span class="line">tree_reg.fit(X, y)</span><br><span class="line">#print(tree_reg.fit(X, y))</span><br></pre></td></tr></table></figure>
</li>
<li><p>两个决策树回归模型的预测对比</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">tree_reg1 &#x3D; DecisionTreeRegressor(random_state&#x3D;42, max_depth&#x3D;2)</span><br><span class="line">tree_reg2 &#x3D; DecisionTreeRegressor(random_state&#x3D;42, max_depth&#x3D;3)</span><br><span class="line">tree_reg1.fit(X, y)</span><br><span class="line">tree_reg2.fit(X, y)</span><br><span class="line"></span><br><span class="line">def plot_regression_predictions(tree_reg, X, y, axes&#x3D;[0, 1, -0.2, 1], ylabel&#x3D;&quot;$y$&quot;):</span><br><span class="line">    x1 &#x3D; np.linspace(axes[0], axes[1], 500).reshape(-1, 1)</span><br><span class="line">    y_pred &#x3D; tree_reg.predict(x1)</span><br><span class="line">    plt.axis(axes)</span><br><span class="line">    plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">    if ylabel:</span><br><span class="line">        plt.ylabel(ylabel, fontsize&#x3D;18, rotation&#x3D;0)</span><br><span class="line">    plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">    plt.plot(x1, y_pred, &quot;r.-&quot;, linewidth&#x3D;2, label&#x3D;r&quot;$\hat&#123;y&#125;$&quot;)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(11, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_regression_predictions(tree_reg1, X, y)</span><br><span class="line">for split, style in ((0.1973, &quot;k-&quot;), (0.0917, &quot;k--&quot;), (0.7718, &quot;k--&quot;)):</span><br><span class="line">    plt.plot([split, split], [-0.2, 1], style, linewidth&#x3D;2)</span><br><span class="line">plt.text(0.21, 0.65, &quot;Depth&#x3D;0&quot;, fontsize&#x3D;15)</span><br><span class="line">plt.text(0.01, 0.2, &quot;Depth&#x3D;1&quot;, fontsize&#x3D;13)</span><br><span class="line">plt.text(0.65, 0.8, &quot;Depth&#x3D;1&quot;, fontsize&#x3D;13)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper center&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.title(&quot;max_depth&#x3D;2&quot;, fontsize&#x3D;14)</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_regression_predictions(tree_reg2, X, y, ylabel&#x3D;None)</span><br><span class="line">for split, style in ((0.1973, &quot;k-&quot;), (0.0917, &quot;k--&quot;), (0.7718, &quot;k--&quot;)):</span><br><span class="line">    plt.plot([split, split], [-0.2, 1], style, linewidth&#x3D;2)</span><br><span class="line">for split in (0.0458, 0.1298, 0.2873, 0.9040):</span><br><span class="line">    plt.plot([split, split], [-0.2, 1], &quot;k:&quot;, linewidth&#x3D;1)</span><br><span class="line">plt.text(0.3, 0.5, &quot;Depth&#x3D;2&quot;, fontsize&#x3D;13)</span><br><span class="line">plt.title(&quot;max_depth&#x3D;3&quot;, fontsize&#x3D;14)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;tree_regression_plot两个决策树回归模型的预测对比&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="不稳定性"><a href="#不稳定性" class="headerlink" title="不稳定性"></a>不稳定性</h4><ol start="7">
<li><p>对数据旋转敏感</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(6)</span><br><span class="line">Xs &#x3D; np.random.rand(100, 2) - 0.5</span><br><span class="line">ys &#x3D; (Xs[:, 0] &gt; 0).astype(np.float32) * 2</span><br><span class="line"></span><br><span class="line">angle &#x3D; np.pi &#x2F; 4</span><br><span class="line">rotation_matrix &#x3D; np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])</span><br><span class="line">Xsr &#x3D; Xs.dot(rotation_matrix)</span><br><span class="line"></span><br><span class="line">tree_clf_s &#x3D; DecisionTreeClassifier(random_state&#x3D;42)</span><br><span class="line">tree_clf_s.fit(Xs, ys)</span><br><span class="line">tree_clf_sr &#x3D; DecisionTreeClassifier(random_state&#x3D;42)</span><br><span class="line">tree_clf_sr.fit(Xsr, ys)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(11, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_decision_boundary(tree_clf_s, Xs, ys, axes&#x3D;[-0.7, 0.7, -0.7, 0.7], iris&#x3D;False)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_decision_boundary(tree_clf_sr, Xsr, ys, axes&#x3D;[-0.7, 0.7, -0.7, 0.7], iris&#x3D;False)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;sensitivity_to_rotation_plot对数据旋转敏感&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>对训练集细节敏感</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">X[(X[:, 1]&#x3D;&#x3D;X[:, 1][y&#x3D;&#x3D;1].max()) &amp; (y&#x3D;&#x3D;1)] # widest Iris-Versicolor flower</span><br><span class="line"></span><br><span class="line">not_widest_versicolor &#x3D; (X[:, 1]!&#x3D;1.8) | (y&#x3D;&#x3D;2)</span><br><span class="line">X_tweaked &#x3D; X[not_widest_versicolor]</span><br><span class="line">y_tweaked &#x3D; y[not_widest_versicolor]</span><br><span class="line"></span><br><span class="line">tree_clf_tweaked &#x3D; DecisionTreeClassifier(max_depth&#x3D;2, random_state&#x3D;40)</span><br><span class="line">tree_clf_tweaked.fit(X_tweaked, y_tweaked)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8, 4))</span><br><span class="line">plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked, legend&#x3D;False)</span><br><span class="line">plt.plot([0, 7.5], [0.8, 0.8], &quot;k-&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot([0, 7.5], [1.75, 1.75], &quot;k--&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.text(1.0, 0.9, &quot;Depth&#x3D;0&quot;, fontsize&#x3D;15)</span><br><span class="line">plt.text(1.0, 1.80, &quot;Depth&#x3D;1&quot;, fontsize&#x3D;13)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;decision_tree_inst    ability_plot对训练集细节敏感&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>





</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-第5章支持向量机" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/28/%E7%AC%AC5%E7%AB%A0%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-date">
      <time datetime="2020-03-28T07:51:40.000Z" itemprop="datePublished">2020-03-28</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/28/%E7%AC%AC5%E7%AB%A0%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">第5章支持向量机</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考：作者的<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/tree/master/" target="_blank" rel="noopener">Jupyter Notebook</a><br><a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/05_support_vector_machines.ipynb" target="_blank" rel="noopener">Chapter 5 – Support Vector Machines</a></p>
<p>支持向量机（简称SVM）是一个功能强大并且全面的机器学习模型，它能够执行线性或非线性分类、回归，甚至是异常值检测任务。它是机器学习领域最受欢迎的模型之一，任何对机器学习感兴趣的人都应该在工具箱中配备一个。SVM特别适用于中小型复杂数据集的分类。</p>
<ol>
<li>保存图片 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import division, print_function, unicode_literals</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import os</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">mpl.rc(&#39;axes&#39;, labelsize&#x3D;14)</span><br><span class="line">mpl.rc(&#39;xtick&#39;, labelsize&#x3D;12)</span><br><span class="line">mpl.rc(&#39;ytick&#39;, labelsize&#x3D;12)</span><br><span class="line"></span><br><span class="line"># Where to save the figures</span><br><span class="line">PROJECT_ROOT_DIR &#x3D; &quot;images&quot;</span><br><span class="line">CHAPTER_ID &#x3D; &quot;traininglinearmodels&quot;</span><br><span class="line"></span><br><span class="line">def save_fig(fig_id, tight_layout&#x3D;True):</span><br><span class="line">    path &#x3D; os.path.join(PROJECT_ROOT_DIR, CHAPTER_ID, fig_id + &quot;.png&quot;)</span><br><span class="line">    print(&quot;Saving figure&quot;, fig_id)</span><br><span class="line">    if tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format&#x3D;&#39;png&#39;, dpi&#x3D;600)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="线性SVM分类"><a href="#线性SVM分类" class="headerlink" title="线性SVM分类"></a>线性SVM分类</h4><ol start="2">
<li>加载鸢尾花数据集，缩放特征，然后训练一个线性SVM模型（使用LinearSVC类，C=0.1，用即将介绍的hinge损失函数）用来检测Virginica鸢尾花。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y &#x3D; iris[&quot;target&quot;]</span><br><span class="line"></span><br><span class="line">setosa_or_versicolor &#x3D; (y &#x3D;&#x3D; 0) | (y &#x3D;&#x3D; 1)</span><br><span class="line">X &#x3D; X[setosa_or_versicolor]</span><br><span class="line">y &#x3D; y[setosa_or_versicolor]</span><br><span class="line"></span><br><span class="line"># SVM Classifier model</span><br><span class="line">svm_clf &#x3D; SVC(kernel&#x3D;&quot;linear&quot;, C&#x3D;float(&quot;inf&quot;))</span><br><span class="line">print(svm_clf.fit(X, y))</span><br><span class="line"></span><br><span class="line"># Bad models</span><br><span class="line">x0 &#x3D; np.linspace(0, 5.5, 200)</span><br><span class="line">pred_1 &#x3D; 5*x0 - 20</span><br><span class="line">pred_2 &#x3D; x0 - 1.8</span><br><span class="line">pred_3 &#x3D; 0.1 * x0 + 0.5</span><br><span class="line"></span><br><span class="line">def plot_svc_decision_boundary(svm_clf, xmin, xmax):</span><br><span class="line">    w &#x3D; svm_clf.coef_[0]</span><br><span class="line">    b &#x3D; svm_clf.intercept_[0]</span><br><span class="line"></span><br><span class="line">    # At the decision boundary, w0*x0 + w1*x1 + b &#x3D; 0</span><br><span class="line">    # &#x3D;&gt; x1 &#x3D; -w0&#x2F;w1 * x0 - b&#x2F;w1</span><br><span class="line">    x0 &#x3D; np.linspace(xmin, xmax, 200)</span><br><span class="line">    decision_boundary &#x3D; -w[0]&#x2F;w[1] * x0 - b&#x2F;w[1]</span><br><span class="line"></span><br><span class="line">    margin &#x3D; 1&#x2F;w[1]</span><br><span class="line">    gutter_up &#x3D; decision_boundary + margin</span><br><span class="line">    gutter_down &#x3D; decision_boundary - margin</span><br><span class="line"></span><br><span class="line">    svs &#x3D; svm_clf.support_vectors_</span><br><span class="line">    plt.scatter(svs[:, 0], svs[:, 1], s&#x3D;180, facecolors&#x3D;&#39;#FFAAAA&#39;)</span><br><span class="line">    plt.plot(x0, decision_boundary, &quot;k-&quot;, linewidth&#x3D;2)</span><br><span class="line">    plt.plot(x0, gutter_up, &quot;k--&quot;, linewidth&#x3D;2)</span><br><span class="line">    plt.plot(x0, gutter_down, &quot;k--&quot;, linewidth&#x3D;2)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(12,2.7))</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x0, pred_1, &quot;g--&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(x0, pred_2, &quot;m-&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(x0, pred_3, &quot;r-&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(X[:, 0][y&#x3D;&#x3D;1], X[:, 1][y&#x3D;&#x3D;1], &quot;bs&quot;, label&#x3D;&quot;Iris-Versicolor&quot;)</span><br><span class="line">plt.plot(X[:, 0][y&#x3D;&#x3D;0], X[:, 1][y&#x3D;&#x3D;0], &quot;yo&quot;, label&#x3D;&quot;Iris-Setosa&quot;)</span><br><span class="line">plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Petal width&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 5.5, 0, 2])</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_svc_decision_boundary(svm_clf, 0, 5.5)</span><br><span class="line">plt.plot(X[:, 0][y&#x3D;&#x3D;1], X[:, 1][y&#x3D;&#x3D;1], &quot;bs&quot;)</span><br><span class="line">plt.plot(X[:, 0][y&#x3D;&#x3D;0], X[:, 1][y&#x3D;&#x3D;0], &quot;yo&quot;)</span><br><span class="line">plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 5.5, 0, 2])</span><br><span class="line"></span><br><span class="line">save_fig(&quot;large_margin_classification_plot较少间隔违例和大间隔对比&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="非线性SVM分类"><a href="#非线性SVM分类" class="headerlink" title="非线性SVM分类"></a>非线性SVM分类</h4><ol start="3">
<li><p>通过添加特征使数据集线性可分离</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">X1D &#x3D; np.linspace(-4, 4, 9).reshape(-1, 1)</span><br><span class="line">X2D &#x3D; np.c_[X1D, X1D**2]</span><br><span class="line">y &#x3D; np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">plt.figure(figsize&#x3D;(11, 4))</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.grid(True, which&#x3D;&#39;both&#39;)</span><br><span class="line">plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.plot(X1D[:, 0][y&#x3D;&#x3D;0], np.zeros(4), &quot;bs&quot;)</span><br><span class="line">plt.plot(X1D[:, 0][y&#x3D;&#x3D;1], np.zeros(5), &quot;g^&quot;)</span><br><span class="line">plt.gca().get_yaxis().set_ticks([])</span><br><span class="line">plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.axis([-4.5, 4.5, -0.2, 0.2])</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.grid(True, which&#x3D;&#39;both&#39;)</span><br><span class="line">plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.plot(X2D[:, 0][y&#x3D;&#x3D;0], X2D[:, 1][y&#x3D;&#x3D;0], &quot;bs&quot;)</span><br><span class="line">plt.plot(X2D[:, 0][y&#x3D;&#x3D;1], X2D[:, 1][y&#x3D;&#x3D;1], &quot;g^&quot;)</span><br><span class="line">plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.ylabel(r&quot;$x_2$&quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line">plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])</span><br><span class="line">plt.plot([-4.5, 4.5], [6.5, 6.5], &quot;r--&quot;, linewidth&#x3D;3)</span><br><span class="line">plt.axis([-4.5, 4.5, -1, 17])</span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(right&#x3D;1)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;higher_dimensions_plot&quot;, tight_layout&#x3D;False)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>要使用Scikit-Learn实现这个想法，可以搭建一条流水线：一个PolynomialFeatures转换器，接着一个StandardScaler，然后是LinearSVC。我们用卫星数据集来测试一下</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.svm import LinearSVC</span><br><span class="line">from sklearn.datasets import make_moons</span><br><span class="line">X, y &#x3D; make_moons(n_samples&#x3D;100, noise&#x3D;0.15, random_state&#x3D;42)</span><br><span class="line"></span><br><span class="line">def plot_dataset(X, y, axes):</span><br><span class="line">    plt.plot(X[:, 0][y&#x3D;&#x3D;0], X[:, 1][y&#x3D;&#x3D;0], &quot;bs&quot;)</span><br><span class="line">    plt.plot(X[:, 0][y&#x3D;&#x3D;1], X[:, 1][y&#x3D;&#x3D;1], &quot;g^&quot;)</span><br><span class="line">    plt.axis(axes)</span><br><span class="line">    plt.grid(True, which&#x3D;&#39;both&#39;)</span><br><span class="line">    plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;20)</span><br><span class="line">    plt.ylabel(r&quot;$x_2$&quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line"></span><br><span class="line">#plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line">from sklearn.datasets import make_moons</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">polynomial_svm_clf &#x3D; Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;3)),</span><br><span class="line">        (&quot;scaler&quot;, StandardScaler()),</span><br><span class="line">        (&quot;svm_clf&quot;, LinearSVC(C&#x3D;10, loss&#x3D;&quot;hinge&quot;, random_state&#x3D;42))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">polynomial_svm_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">def plot_predictions(clf, axes):</span><br><span class="line">    x0s &#x3D; np.linspace(axes[0], axes[1], 100)</span><br><span class="line">    x1s &#x3D; np.linspace(axes[2], axes[3], 100)</span><br><span class="line">    x0, x1 &#x3D; np.meshgrid(x0s, x1s)</span><br><span class="line">    X &#x3D; np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line">    y_pred &#x3D; clf.predict(X).reshape(x0.shape)</span><br><span class="line">    y_decision &#x3D; clf.decision_function(X).reshape(x0.shape)</span><br><span class="line">    plt.contourf(x0, x1, y_pred, cmap&#x3D;plt.cm.brg, alpha&#x3D;0.2)</span><br><span class="line">    plt.contourf(x0, x1, y_decision, cmap&#x3D;plt.cm.brg, alpha&#x3D;0.1)</span><br><span class="line"></span><br><span class="line">plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])</span><br><span class="line"></span><br><span class="line">save_fig(&quot;moons_polynomial_svc_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>多项式核</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line">poly_kernel_svm_clf &#x3D; Pipeline([</span><br><span class="line">        (&quot;scaler&quot;, StandardScaler()),</span><br><span class="line">        (&quot;svm_clf&quot;, SVC(kernel&#x3D;&quot;poly&quot;, degree&#x3D;3, coef0&#x3D;1, C&#x3D;5))</span><br><span class="line">    ])</span><br><span class="line">poly_kernel_svm_clf.fit(X, y)</span><br><span class="line">#print(poly_kernel_svm_clf.fit(X, y))</span><br><span class="line"></span><br><span class="line">poly100_kernel_svm_clf &#x3D; Pipeline([</span><br><span class="line">        (&quot;scaler&quot;, StandardScaler()),</span><br><span class="line">        (&quot;svm_clf&quot;, SVC(kernel&#x3D;&quot;poly&quot;, degree&#x3D;10, coef0&#x3D;100, C&#x3D;5))</span><br><span class="line">    ])</span><br><span class="line">poly100_kernel_svm_clf.fit(X, y)</span><br><span class="line">#print(poly100_kernel_svm_clf.fit(X, y))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(11, 4))</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">plt.title(r&quot;$d&#x3D;3, r&#x3D;1, C&#x3D;5$&quot;, fontsize&#x3D;18)</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">plt.title(r&quot;$d&#x3D;10, r&#x3D;100, C&#x3D;5$&quot;, fontsize&#x3D;18)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;moons_kernelized_polynomial_svc_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加相似特征</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">def gaussian_rbf(x, landmark, gamma):</span><br><span class="line">    return np.exp(-gamma * np.linalg.norm(x - landmark, axis&#x3D;1)**2)</span><br><span class="line"></span><br><span class="line">gamma &#x3D; 0.3</span><br><span class="line"></span><br><span class="line">x1s &#x3D; np.linspace(-4.5, 4.5, 200).reshape(-1, 1)</span><br><span class="line">x2s &#x3D; gaussian_rbf(x1s, -2, gamma)</span><br><span class="line">x3s &#x3D; gaussian_rbf(x1s, 1, gamma)</span><br><span class="line"></span><br><span class="line">XK &#x3D; np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]</span><br><span class="line">yk &#x3D; np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(11, 4))</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.grid(True, which&#x3D;&#39;both&#39;)</span><br><span class="line">plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.scatter(x&#x3D;[-2, 1], y&#x3D;[0, 0], s&#x3D;150, alpha&#x3D;0.5, c&#x3D;&quot;red&quot;)</span><br><span class="line">plt.plot(X1D[:, 0][yk&#x3D;&#x3D;0], np.zeros(4), &quot;bs&quot;)</span><br><span class="line">plt.plot(X1D[:, 0][yk&#x3D;&#x3D;1], np.zeros(5), &quot;g^&quot;)</span><br><span class="line">plt.plot(x1s, x2s, &quot;g--&quot;)</span><br><span class="line">plt.plot(x1s, x3s, &quot;b:&quot;)</span><br><span class="line">plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])</span><br><span class="line">plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.ylabel(r&quot;Similarity&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.annotate(r&#39;$\mathbf&#123;x&#125;$&#39;,</span><br><span class="line">            xy&#x3D;(X1D[3, 0], 0),</span><br><span class="line">            xytext&#x3D;(-0.5, 0.20),</span><br><span class="line">            ha&#x3D;&quot;center&quot;,</span><br><span class="line">            arrowprops&#x3D;dict(facecolor&#x3D;&#39;black&#39;, shrink&#x3D;0.1),</span><br><span class="line">            fontsize&#x3D;18,</span><br><span class="line">            )</span><br><span class="line">plt.text(-2, 0.9, &quot;$x_2$&quot;, ha&#x3D;&quot;center&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.text(1, 0.9, &quot;$x_3$&quot;, ha&#x3D;&quot;center&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.axis([-4.5, 4.5, -0.1, 1.1])</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.grid(True, which&#x3D;&#39;both&#39;)</span><br><span class="line">plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.plot(XK[:, 0][yk&#x3D;&#x3D;0], XK[:, 1][yk&#x3D;&#x3D;0], &quot;bs&quot;)</span><br><span class="line">plt.plot(XK[:, 0][yk&#x3D;&#x3D;1], XK[:, 1][yk&#x3D;&#x3D;1], &quot;g^&quot;)</span><br><span class="line">plt.xlabel(r&quot;$x_2$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.ylabel(r&quot;$x_3$  &quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line">plt.annotate(r&#39;$\phi\left(\mathbf&#123;x&#125;\right)$&#39;,</span><br><span class="line">            xy&#x3D;(XK[3, 0], XK[3, 1]),</span><br><span class="line">            xytext&#x3D;(0.65, 0.50),</span><br><span class="line">            ha&#x3D;&quot;center&quot;,</span><br><span class="line">            arrowprops&#x3D;dict(facecolor&#x3D;&#39;black&#39;, shrink&#x3D;0.1),</span><br><span class="line">            fontsize&#x3D;18,</span><br><span class="line">            )</span><br><span class="line">plt.plot([-0.1, 1.1], [0.57, -0.1], &quot;r--&quot;, linewidth&#x3D;3)</span><br><span class="line">plt.axis([-0.1, 1.1, -0.1, 1.1])</span><br><span class="line">    </span><br><span class="line">plt.subplots_adjust(right&#x3D;1)</span><br><span class="line"></span><br><span class="line">#save_fig(&quot;kernel_method_plot&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>高斯RBF</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x1_example &#x3D; X1D[3, 0]</span><br><span class="line">for landmark in (-2, 1):</span><br><span class="line">    k &#x3D; gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)</span><br><span class="line">    print(&quot;Phi(&#123;&#125;, &#123;&#125;) &#x3D; &#123;&#125;&quot;.format(x1_example, landmark, k))</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用SVC类试试高斯RBF核</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">rbf_kernel_svm_clf &#x3D; Pipeline([</span><br><span class="line">        (&quot;scaler&quot;, StandardScaler()),</span><br><span class="line">        (&quot;svm_clf&quot;, SVC(kernel&#x3D;&quot;rbf&quot;, gamma&#x3D;5, C&#x3D;0.001))</span><br><span class="line">    ])</span><br><span class="line">rbf_kernel_svm_clf.fit(X, y)</span><br><span class="line">print(rbf_kernel_svm_clf.fit(X, y))</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line">gamma1, gamma2 &#x3D; 0.1, 5</span><br><span class="line">C1, C2 &#x3D; 0.001, 1000</span><br><span class="line">hyperparams &#x3D; (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)</span><br><span class="line"></span><br><span class="line">svm_clfs &#x3D; []</span><br><span class="line">for gamma, C in hyperparams:</span><br><span class="line">    rbf_kernel_svm_clf &#x3D; Pipeline([</span><br><span class="line">            (&quot;scaler&quot;, StandardScaler()),</span><br><span class="line">            (&quot;svm_clf&quot;, SVC(kernel&#x3D;&quot;rbf&quot;, gamma&#x3D;gamma, C&#x3D;C))</span><br><span class="line">        ])</span><br><span class="line">    rbf_kernel_svm_clf.fit(X, y)</span><br><span class="line">    svm_clfs.append(rbf_kernel_svm_clf)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(11, 7))</span><br><span class="line"></span><br><span class="line">for i, svm_clf in enumerate(svm_clfs):</span><br><span class="line">    plt.subplot(221 + i)</span><br><span class="line">    plot_predictions(svm_clf, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">    plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])</span><br><span class="line">    gamma, C &#x3D; hyperparams[i]</span><br><span class="line">    plt.title(r&quot;$\gamma &#x3D; &#123;&#125;, C &#x3D; &#123;&#125;$&quot;.format(gamma, C), fontsize&#x3D;16)</span><br><span class="line">#使用RBF核的SVM分类器</span><br><span class="line">save_fig(&quot;moons_rbf_svc_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="SVM回归"><a href="#SVM回归" class="headerlink" title="SVM回归"></a>SVM回归</h4><ol start="9">
<li><p>SVM回归</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 50</span><br><span class="line">X &#x3D; 2 * np.random.rand(m, 1)</span><br><span class="line">y &#x3D; (4 + 3 * X + np.random.randn(m, 1)).ravel()</span><br><span class="line"></span><br><span class="line">from sklearn.svm import LinearSVR</span><br><span class="line"></span><br><span class="line">svm_reg &#x3D; LinearSVR(epsilon&#x3D;1.5, random_state&#x3D;42)</span><br><span class="line">svm_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">svm_reg1 &#x3D; LinearSVR(epsilon&#x3D;1.5, random_state&#x3D;42)</span><br><span class="line">svm_reg2 &#x3D; LinearSVR(epsilon&#x3D;0.5, random_state&#x3D;42)</span><br><span class="line">svm_reg1.fit(X, y)</span><br><span class="line">svm_reg2.fit(X, y)</span><br><span class="line"></span><br><span class="line">def find_support_vectors(svm_reg, X, y):</span><br><span class="line">    y_pred &#x3D; svm_reg.predict(X)</span><br><span class="line">    off_margin &#x3D; (np.abs(y - y_pred) &gt;&#x3D; svm_reg.epsilon)</span><br><span class="line">    return np.argwhere(off_margin)</span><br><span class="line"></span><br><span class="line">svm_reg1.support_ &#x3D; find_support_vectors(svm_reg1, X, y)</span><br><span class="line">svm_reg2.support_ &#x3D; find_support_vectors(svm_reg2, X, y)</span><br><span class="line"></span><br><span class="line">eps_x1 &#x3D; 1</span><br><span class="line">eps_y_pred &#x3D; svm_reg1.predict([[eps_x1]])</span><br><span class="line"></span><br><span class="line">def plot_svm_regression(svm_reg, X, y, axes):</span><br><span class="line">    x1s &#x3D; np.linspace(axes[0], axes[1], 100).reshape(100, 1)</span><br><span class="line">    y_pred &#x3D; svm_reg.predict(x1s)</span><br><span class="line">    plt.plot(x1s, y_pred, &quot;k-&quot;, linewidth&#x3D;2, label&#x3D;r&quot;$\hat&#123;y&#125;$&quot;)</span><br><span class="line">    plt.plot(x1s, y_pred + svm_reg.epsilon, &quot;k--&quot;)</span><br><span class="line">    plt.plot(x1s, y_pred - svm_reg.epsilon, &quot;k--&quot;)</span><br><span class="line">    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s&#x3D;180, facecolors&#x3D;&#39;#FFAAAA&#39;)</span><br><span class="line">    plt.plot(X, y, &quot;bo&quot;)</span><br><span class="line">    plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;18)</span><br><span class="line">    plt.axis(axes)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(9, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])</span><br><span class="line">plt.title(r&quot;$\epsilon &#x3D; &#123;&#125;$&quot;.format(svm_reg1.epsilon), fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(r&quot;$y$&quot;, fontsize&#x3D;18, rotation&#x3D;0)</span><br><span class="line">#plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], &quot;k-&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.annotate(</span><br><span class="line">        &#39;&#39;, xy&#x3D;(eps_x1, eps_y_pred), xycoords&#x3D;&#39;data&#39;,</span><br><span class="line">        xytext&#x3D;(eps_x1, eps_y_pred - svm_reg1.epsilon),</span><br><span class="line">        textcoords&#x3D;&#39;data&#39;, arrowprops&#x3D;&#123;&#39;arrowstyle&#39;: &#39;&lt;-&gt;&#39;, &#39;linewidth&#39;: 1.5&#125;</span><br><span class="line">    )</span><br><span class="line">plt.text(0.91, 5.6, r&quot;$\epsilon$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])</span><br><span class="line">plt.title(r&quot;$\epsilon &#x3D; &#123;&#125;$&quot;.format(svm_reg2.epsilon), fontsize&#x3D;18)</span><br><span class="line">save_fig(&quot;svm_regression_plot使用二阶多项式核的SVM回归&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用二阶多项式核的SVM回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 100</span><br><span class="line">X &#x3D; 2 * np.random.rand(m, 1) - 1</span><br><span class="line">y &#x3D; (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)&#x2F;10).ravel()</span><br><span class="line">#SVR类是SVC类的回归等价物，LinearSVR类也是LinearSVC类的回归等价物。LinearSVR与训练集的大小线性相关</span><br><span class="line">#（跟LinearSVC一样），而SVR则在训练集变大时，变得很慢（SVC也是一样）。</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVR</span><br><span class="line"></span><br><span class="line">svm_poly_reg1 &#x3D; SVR(kernel&#x3D;&quot;poly&quot;, degree&#x3D;2, C&#x3D;100, epsilon&#x3D;0.1, gamma&#x3D;&quot;auto&quot;)</span><br><span class="line">svm_poly_reg2 &#x3D; SVR(kernel&#x3D;&quot;poly&quot;, degree&#x3D;2, C&#x3D;0.01, epsilon&#x3D;0.1, gamma&#x3D;&quot;auto&quot;)</span><br><span class="line">svm_poly_reg1.fit(X, y)</span><br><span class="line">svm_poly_reg2.fit(X, y)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(9, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])</span><br><span class="line">plt.title(r&quot;$degree&#x3D;&#123;&#125;, C&#x3D;&#123;&#125;, \epsilon &#x3D; &#123;&#125;$&quot;.format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(r&quot;$y$&quot;, fontsize&#x3D;18, rotation&#x3D;0)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])</span><br><span class="line">plt.title(r&quot;$degree&#x3D;&#123;&#125;, C&#x3D;&#123;&#125;, \epsilon &#x3D; &#123;&#125;$&quot;.format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize&#x3D;18)</span><br><span class="line">save_fig(&quot;svm_with_polynomial_kernel_plot使用二阶多项式核的SVM回归&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>鸢尾花数据集的决策函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; StandardScaler()</span><br><span class="line">svm_clf1 &#x3D; LinearSVC(C&#x3D;1, loss&#x3D;&quot;hinge&quot;, random_state&#x3D;42)</span><br><span class="line">svm_clf2 &#x3D; LinearSVC(C&#x3D;100, loss&#x3D;&quot;hinge&quot;, random_state&#x3D;42)</span><br><span class="line"></span><br><span class="line">scaled_svm_clf1 &#x3D; Pipeline([</span><br><span class="line">        (&quot;scaler&quot;, scaler),</span><br><span class="line">        (&quot;linear_svc&quot;, svm_clf1),</span><br><span class="line">    ])</span><br><span class="line">scaled_svm_clf2 &#x3D; Pipeline([</span><br><span class="line">        (&quot;scaler&quot;, scaler),</span><br><span class="line">        (&quot;linear_svc&quot;, svm_clf2),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">scaled_svm_clf1.fit(X, y)</span><br><span class="line">scaled_svm_clf2.fit(X, y)</span><br><span class="line"></span><br><span class="line"># Convert to unscaled parameters</span><br><span class="line">b1 &#x3D; svm_clf1.decision_function([-scaler.mean_ &#x2F; scaler.scale_])</span><br><span class="line">b2 &#x3D; svm_clf2.decision_function([-scaler.mean_ &#x2F; scaler.scale_])</span><br><span class="line">w1 &#x3D; svm_clf1.coef_[0] &#x2F; scaler.scale_</span><br><span class="line">w2 &#x3D; svm_clf2.coef_[0] &#x2F; scaler.scale_</span><br><span class="line">svm_clf1.intercept_ &#x3D; np.array([b1])</span><br><span class="line">svm_clf2.intercept_ &#x3D; np.array([b2])</span><br><span class="line">svm_clf1.coef_ &#x3D; np.array([w1])</span><br><span class="line">svm_clf2.coef_ &#x3D; np.array([w2])</span><br><span class="line"></span><br><span class="line"># Find support vectors (LinearSVC does not do this automatically)</span><br><span class="line">t &#x3D; y * 2 - 1</span><br><span class="line">support_vectors_idx1 &#x3D; (t * (X.dot(w1) + b1) &lt; 1).ravel()</span><br><span class="line">support_vectors_idx2 &#x3D; (t * (X.dot(w2) + b2) &lt; 1).ravel()</span><br><span class="line">svm_clf1.support_vectors_ &#x3D; X[support_vectors_idx1]</span><br><span class="line">svm_clf2.support_vectors_ &#x3D; X[support_vectors_idx2]</span><br><span class="line"></span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y &#x3D; (iris[&quot;target&quot;] &#x3D;&#x3D; 2).astype(np.float64)  # Iris-Virginica</span><br><span class="line"></span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line">def plot_3D_decision_function(ax, w, b, x1_lim&#x3D;[4, 6], x2_lim&#x3D;[0.8, 2.8]):</span><br><span class="line">    x1_in_bounds &#x3D; (X[:, 0] &gt; x1_lim[0]) &amp; (X[:, 0] &lt; x1_lim[1])</span><br><span class="line">    X_crop &#x3D; X[x1_in_bounds]</span><br><span class="line">    y_crop &#x3D; y[x1_in_bounds]</span><br><span class="line">    x1s &#x3D; np.linspace(x1_lim[0], x1_lim[1], 20)</span><br><span class="line">    x2s &#x3D; np.linspace(x2_lim[0], x2_lim[1], 20)</span><br><span class="line">    x1, x2 &#x3D; np.meshgrid(x1s, x2s)</span><br><span class="line">    xs &#x3D; np.c_[x1.ravel(), x2.ravel()]</span><br><span class="line">    df &#x3D; (xs.dot(w) + b).reshape(x1.shape)</span><br><span class="line">    m &#x3D; 1 &#x2F; np.linalg.norm(w)</span><br><span class="line">    boundary_x2s &#x3D; -x1s*(w[0]&#x2F;w[1])-b&#x2F;w[1]</span><br><span class="line">    margin_x2s_1 &#x3D; -x1s*(w[0]&#x2F;w[1])-(b-1)&#x2F;w[1]</span><br><span class="line">    margin_x2s_2 &#x3D; -x1s*(w[0]&#x2F;w[1])-(b+1)&#x2F;w[1]</span><br><span class="line">    ax.plot_surface(x1s, x2, np.zeros_like(x1),</span><br><span class="line">                    color&#x3D;&quot;b&quot;, alpha&#x3D;0.2, cstride&#x3D;100, rstride&#x3D;100)</span><br><span class="line">    ax.plot(x1s, boundary_x2s, 0, &quot;k-&quot;, linewidth&#x3D;2, label&#x3D;r&quot;$h&#x3D;0$&quot;)</span><br><span class="line">    ax.plot(x1s, margin_x2s_1, 0, &quot;k--&quot;, linewidth&#x3D;2, label&#x3D;r&quot;$h&#x3D;\pm 1$&quot;)</span><br><span class="line">    ax.plot(x1s, margin_x2s_2, 0, &quot;k--&quot;, linewidth&#x3D;2)</span><br><span class="line">    ax.plot(X_crop[:, 0][y_crop&#x3D;&#x3D;1], X_crop[:, 1][y_crop&#x3D;&#x3D;1], 0, &quot;g^&quot;)</span><br><span class="line">    ax.plot_wireframe(x1, x2, df, alpha&#x3D;0.3, color&#x3D;&quot;k&quot;)</span><br><span class="line">    ax.plot(X_crop[:, 0][y_crop&#x3D;&#x3D;0], X_crop[:, 1][y_crop&#x3D;&#x3D;0], 0, &quot;bs&quot;)</span><br><span class="line">    ax.axis(x1_lim + x2_lim)</span><br><span class="line">    ax.text(4.5, 2.5, 3.8, &quot;Decision function $h$&quot;, fontsize&#x3D;15)</span><br><span class="line">    ax.set_xlabel(r&quot;Petal length&quot;, fontsize&#x3D;15)</span><br><span class="line">    ax.set_ylabel(r&quot;Petal width&quot;, fontsize&#x3D;15)</span><br><span class="line">    ax.set_zlabel(r&quot;$h &#x3D; \mathbf&#123;w&#125;^T \mathbf&#123;x&#125; + b$&quot;, fontsize&#x3D;18)</span><br><span class="line">    ax.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;16)</span><br><span class="line"></span><br><span class="line">fig &#x3D; plt.figure(figsize&#x3D;(11, 6))</span><br><span class="line">ax1 &#x3D; fig.add_subplot(111, projection&#x3D;&#39;3d&#39;)</span><br><span class="line">plot_3D_decision_function(ax1, w&#x3D;svm_clf2.coef_[0], b&#x3D;svm_clf2.intercept_[0])</span><br><span class="line"></span><br><span class="line">#save_fig(&quot;iris_3D_plot鸢尾花数据集的决策函数&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>权重向量越小，间隔越大</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">def plot_2D_decision_function(w, b, ylabel&#x3D;True, x1_lim&#x3D;[-3, 3]):</span><br><span class="line">    x1 &#x3D; np.linspace(x1_lim[0], x1_lim[1], 200)</span><br><span class="line">    y &#x3D; w * x1 + b</span><br><span class="line">    m &#x3D; 1 &#x2F; w</span><br><span class="line"></span><br><span class="line">    plt.plot(x1, y)</span><br><span class="line">    plt.plot(x1_lim, [1, 1], &quot;k:&quot;)</span><br><span class="line">    plt.plot(x1_lim, [-1, -1], &quot;k:&quot;)</span><br><span class="line">    plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.plot([m, m], [0, 1], &quot;k--&quot;)</span><br><span class="line">    plt.plot([-m, -m], [0, -1], &quot;k--&quot;)</span><br><span class="line">    plt.plot([-m, m], [0, 0], &quot;k-o&quot;, linewidth&#x3D;3)</span><br><span class="line">    plt.axis(x1_lim + [-2, 2])</span><br><span class="line">    plt.xlabel(r&quot;$x_1$&quot;, fontsize&#x3D;16)</span><br><span class="line">    if ylabel:</span><br><span class="line">        plt.ylabel(r&quot;$w_1 x_1$  &quot;, rotation&#x3D;0, fontsize&#x3D;16)</span><br><span class="line">    plt.title(r&quot;$w_1 &#x3D; &#123;&#125;$&quot;.format(w), fontsize&#x3D;16)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(12, 3.2))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_2D_decision_function(1, 0)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_2D_decision_function(0.5, 0, ylabel&#x3D;False)</span><br><span class="line">save_fig(&quot;small_w_large_margin_plot&quot;)</span><br><span class="line">plt.show()</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)] # petal length, petal width</span><br><span class="line">y &#x3D; (iris[&quot;target&quot;] &#x3D;&#x3D; 2).astype(np.float64) # Iris-Virginica</span><br><span class="line"></span><br><span class="line">svm_clf &#x3D; SVC(kernel&#x3D;&quot;linear&quot;, C&#x3D;1)</span><br><span class="line">svm_clf.fit(X, y)</span><br><span class="line">svm_clf.predict([[5.3, 1.3]])</span><br><span class="line">#print(svm_clf.predict([[5.3, 1.3]]))</span><br><span class="line"></span><br><span class="line">#Hinge loss</span><br><span class="line">t &#x3D; np.linspace(-2, 4, 200)</span><br><span class="line">h &#x3D; np.where(1 - t &lt; 0, 0, 1 - t)  # max(0, 1-t)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(5,2.8))</span><br><span class="line">plt.plot(t, h, &quot;b-&quot;, linewidth&#x3D;2, label&#x3D;&quot;$max(0, 1 - t)$&quot;)</span><br><span class="line">plt.grid(True, which&#x3D;&#39;both&#39;)</span><br><span class="line">plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">plt.yticks(np.arange(-1, 2.5, 1))</span><br><span class="line">plt.xlabel(&quot;$t$&quot;, fontsize&#x3D;16)</span><br><span class="line">plt.axis([-2, 4, -1, 2.5])</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper right&quot;, fontsize&#x3D;16)</span><br><span class="line">save_fig(&quot;hinge_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>













</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-第4章训练模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" class="article-date">
      <time datetime="2020-03-26T11:55:09.000Z" itemprop="datePublished">2020-03-26</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/26/%E7%AC%AC4%E7%AB%A0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">第4章训练模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考：作者的<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/tree/master/" target="_blank" rel="noopener">Jupyter Notebook</a><br><a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/04_training_linear_models.ipynb" target="_blank" rel="noopener">Chapter 4 – Training Linear Models</a></p>
<ol>
<li>生成图片并保存 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import division, print_function, unicode_literals</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import os</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">mpl.rc(&#39;axes&#39;, labelsize&#x3D;14)</span><br><span class="line">mpl.rc(&#39;xtick&#39;, labelsize&#x3D;12)</span><br><span class="line">mpl.rc(&#39;ytick&#39;, labelsize&#x3D;12)</span><br><span class="line"></span><br><span class="line"># Where to save the figures</span><br><span class="line">PROJECT_ROOT_DIR &#x3D; &quot;images&quot;</span><br><span class="line">CHAPTER_ID &#x3D; &quot;traininglinearmodels&quot;</span><br><span class="line"></span><br><span class="line">def save_fig(fig_id, tight_layout&#x3D;True):</span><br><span class="line">    path &#x3D; os.path.join(PROJECT_ROOT_DIR, CHAPTER_ID, fig_id + &quot;.png&quot;)</span><br><span class="line">    print(&quot;Saving figure&quot;, fig_id)</span><br><span class="line">    if tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format&#x3D;&#39;png&#39;, dpi&#x3D;600)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><ol start="2">
<li><p>生成一些线性数据来测试这个公式（标准方程）</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">X &#x3D; 2 * np.random.rand(100, 1)</span><br><span class="line">y &#x3D; 4 + 3 * X + np.random.randn(100, 1)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">#save_fig(&quot;generated_data_plot&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用NumPy的线性代数模块（np.linalg）中的inv（）函数来对矩阵求逆，并用dot（）方法计算矩阵的内积：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_b &#x3D; np.c_[np.ones((100, 1)), X]  # add x0 &#x3D; 1 to each instance</span><br><span class="line">theta_best &#x3D; np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line">#print(theta_best)</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以用*做出预测：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">X_new &#x3D; np.array([[0], [2]])</span><br><span class="line">X_new_b &#x3D; np.c_[np.ones((2, 1)), X_new]  # add x0 &#x3D; 1 to each instance</span><br><span class="line">y_predict &#x3D; X_new_b.dot(theta_best)</span><br><span class="line">#print(y_predict)</span><br><span class="line"></span><br><span class="line">#绘制模型的预测结果</span><br><span class="line">plt.plot(X_new, y_predict, &quot;r-&quot;)</span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">plt.plot(X_new, y_predict, &quot;r-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Predictions&quot;)</span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">#save_fig(&quot;linear_model_predictions&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Scikit-Learn的等效代码如下所示</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line">print(lin_reg.intercept_, lin_reg.coef_)</span><br><span class="line">print(lin_reg.predict(X_new))</span><br><span class="line">theta_best_svd, residuals, rank, s &#x3D; np.linalg.lstsq(X_b, y, rcond&#x3D;1e-6)</span><br><span class="line">print(theta_best_svd)</span><br><span class="line">print(np.linalg.pinv(X_b).dot(y))</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间如果学习率太高，那你可能会越过山谷直接到达山的另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案</p>
<ol start="6">
<li><p>批量梯度下降:3个公式，这个算法的快速实现：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eta &#x3D; 0.1</span><br><span class="line">n_iterations &#x3D; 1000</span><br><span class="line">m &#x3D; 100</span><br><span class="line">theta &#x3D; np.random.randn(2,1)</span><br><span class="line"></span><br><span class="line">for iteration in range(n_iterations):</span><br><span class="line">    gradients &#x3D; 2&#x2F;m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">    theta &#x3D; theta - eta * gradients</span><br><span class="line">#print(theta)</span><br><span class="line">#print(X_new_b.dot(theta))</span><br></pre></td></tr></table></figure>
</li>
<li><p>分别使用三种不同的学习率时，梯度下降的前十步:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">theta_path_bgd &#x3D; []</span><br><span class="line">def plot_gradient_descent(theta, eta, theta_path&#x3D;None):</span><br><span class="line">    m &#x3D; len(X_b)</span><br><span class="line">    plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">    n_iterations &#x3D; 1000</span><br><span class="line">    for iteration in range(n_iterations):</span><br><span class="line">        if iteration &lt; 10:</span><br><span class="line">            y_predict &#x3D; X_new_b.dot(theta)</span><br><span class="line">            style &#x3D; &quot;b-&quot; if iteration &gt; 0 else &quot;r--&quot;</span><br><span class="line">            plt.plot(X_new, y_predict, style)</span><br><span class="line">        gradients &#x3D; 2&#x2F;m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        if theta_path is not None:</span><br><span class="line">            theta_path.append(theta)</span><br><span class="line">    plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">    plt.axis([0, 2, 0, 15])</span><br><span class="line">    plt.title(r&quot;$\eta &#x3D; &#123;&#125;$&quot;.format(eta), fontsize&#x3D;16)</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line">theta &#x3D; np.random.randn(2,1)  # random initialization</span><br><span class="line">plt.figure(figsize&#x3D;(10,4))</span><br><span class="line">plt.subplot(131); plot_gradient_descent(theta, eta&#x3D;0.02)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.subplot(132); plot_gradient_descent(theta, eta&#x3D;0.1, theta_path&#x3D;theta_path_bgd)</span><br><span class="line">plt.subplot(133); plot_gradient_descent(theta, eta&#x3D;0.5)</span><br><span class="line">#save_fig(&quot;gradient_descent_plot&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面这段代码使用了一个简单的学习计划实现随机梯度下降：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">theta_path_sgd &#x3D; []</span><br><span class="line">m &#x3D; len(X_b)</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">n_epochs &#x3D; 50</span><br><span class="line">t0, t1 &#x3D; 5, 50  # learning schedule hyperparameters</span><br><span class="line"></span><br><span class="line">def learning_schedule(t):</span><br><span class="line">    return t0 &#x2F; (t + t1)</span><br><span class="line"></span><br><span class="line">theta &#x3D; np.random.randn(2,1)  # random initialization</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    for i in range(m):</span><br><span class="line">        if epoch &#x3D;&#x3D; 0 and i &lt; 20:                    # not shown in the book</span><br><span class="line">            y_predict &#x3D; X_new_b.dot(theta)           # not shown</span><br><span class="line">            style &#x3D; &quot;b-&quot; if i &gt; 0 else &quot;r--&quot;         # not shown</span><br><span class="line">            plt.plot(X_new, y_predict, style)        # not shown</span><br><span class="line">        random_index &#x3D; np.random.randint(m)</span><br><span class="line">        xi &#x3D; X_b[random_index:random_index+1]</span><br><span class="line">        yi &#x3D; y[random_index:random_index+1]</span><br><span class="line">        gradients &#x3D; 2 * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        eta &#x3D; learning_schedule(epoch * m + i)</span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        theta_path_sgd.append(theta)                 # not shown</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)                                 # not shown</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)                     # not shown</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)           # not shown</span><br><span class="line">plt.axis([0, 2, 0, 15])                              # not shown</span><br><span class="line">#save_fig(&quot;sgd_plot&quot;)                                 # not shown</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line">from sklearn.linear_model import SGDRegressor</span><br><span class="line">sgd_reg &#x3D; SGDRegressor(n_iter&#x3D;50, penalty&#x3D;None, eta0&#x3D;0.1)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">print(sgd_reg.intercept_, sgd_reg.coef_)</span><br></pre></td></tr></table></figure>
</li>
<li><p>小批量梯度下降</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">theta_path_bgd &#x3D; []</span><br><span class="line">theta_path_sgd &#x3D; []</span><br><span class="line">theta_path_mgd &#x3D; []</span><br><span class="line">m &#x3D; 100</span><br><span class="line">n_iterations &#x3D; 50</span><br><span class="line">minibatch_size &#x3D; 20</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line">theta &#x3D; np.random.randn(2,1)  # random initialization</span><br><span class="line"></span><br><span class="line">t0, t1 &#x3D; 200, 1000</span><br><span class="line">def learning_schedule(t):</span><br><span class="line">    return t0 &#x2F; (t + t1)</span><br><span class="line"></span><br><span class="line">t &#x3D; 0</span><br><span class="line">for epoch in range(n_iterations):</span><br><span class="line">    shuffled_indices &#x3D; np.random.permutation(m)</span><br><span class="line">    X_b_shuffled &#x3D; X_b[shuffled_indices]</span><br><span class="line">    y_shuffled &#x3D; y[shuffled_indices]</span><br><span class="line">    for i in range(0, m, minibatch_size):</span><br><span class="line">        t +&#x3D; 1</span><br><span class="line">        xi &#x3D; X_b_shuffled[i:i+minibatch_size]</span><br><span class="line">        yi &#x3D; y_shuffled[i:i+minibatch_size]</span><br><span class="line">        gradients &#x3D; 2&#x2F;minibatch_size * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        eta &#x3D; learning_schedule(t)</span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        theta_path_mgd.append(theta)</span><br><span class="line"></span><br><span class="line">theta_path_bgd &#x3D; np.array(theta_path_bgd)</span><br><span class="line">theta_path_sgd &#x3D; np.array(theta_path_sgd)</span><br><span class="line">theta_path_mgd &#x3D; np.array(theta_path_mgd)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(7,4))</span><br><span class="line">plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], &quot;r-s&quot;, linewidth&#x3D;1, label&#x3D;&quot;Stochastic&quot;)</span><br><span class="line">plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], &quot;g-+&quot;, linewidth&#x3D;2, label&#x3D;&quot;Mini-batch&quot;)</span><br><span class="line">plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], &quot;b-o&quot;, linewidth&#x3D;3, label&#x3D;&quot;Batch&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;16)</span><br><span class="line">plt.xlabel(r&quot;$\theta_0$&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.ylabel(r&quot;$\theta_1$   &quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line">plt.axis([2.5, 4.5, 2.3, 3.9])</span><br><span class="line">save_fig(&quot;gradient_descent_paths_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><ol start="10">
<li><p>基于简单的二次方程（注：二次方程的形式为y=ax2+bx+c）制造一些非线性数据（添加随机噪声)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import numpy.random as rnd</span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">m &#x3D; 100</span><br><span class="line">X &#x3D; 6 * np.random.rand(m, 1) - 3</span><br><span class="line">y &#x3D; 0.5 * X**2 + X + 2 + np.random.randn(m, 1)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.axis([-3, 3, 0, 10])</span><br><span class="line">save_fig(&quot;quadratic_data_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Scikit-Learn的PolynomialFeatures类来对训练数据进行转换</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">poly_features &#x3D; PolynomialFeatures(degree&#x3D;2, include_bias&#x3D;False)</span><br><span class="line">X_poly &#x3D; poly_features.fit_transform(X)</span><br><span class="line">#print(X[0])</span><br><span class="line">#print(X_poly[0])</span><br></pre></td></tr></table></figure>
</li>
<li><p>对这个扩展后的训练集匹配一个LinearRegression模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br><span class="line">#print(lin_reg.intercept_, lin_reg.coef_)</span><br><span class="line"></span><br><span class="line">X_new&#x3D;np.linspace(-3, 3, 100).reshape(100, 1)</span><br><span class="line">X_new_poly &#x3D; poly_features.transform(X_new)</span><br><span class="line">y_new &#x3D; lin_reg.predict(X_new_poly)</span><br><span class="line">plt.plot(X, y, &quot;b.&quot;)</span><br><span class="line">plt.plot(X_new, y_new, &quot;r-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Predictions&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([-3, 3, 0, 10])</span><br><span class="line">#save_fig(&quot;quadratic_predictions_plot(多项式回归模型预测)&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><ol start="13">
<li><p>高阶多项式回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">for style, width, degree in ((&quot;g-&quot;, 1, 300), (&quot;b--&quot;, 2, 2), (&quot;r-+&quot;, 2, 1)):</span><br><span class="line">    polybig_features &#x3D; PolynomialFeatures(degree&#x3D;degree, include_bias&#x3D;False)</span><br><span class="line">    std_scaler &#x3D; StandardScaler()</span><br><span class="line">    lin_reg &#x3D; LinearRegression()</span><br><span class="line">    polynomial_regression &#x3D; Pipeline([</span><br><span class="line">            (&quot;poly_features&quot;, polybig_features),</span><br><span class="line">            (&quot;std_scaler&quot;, std_scaler),</span><br><span class="line">            (&quot;lin_reg&quot;, lin_reg),</span><br><span class="line">        ])</span><br><span class="line">    polynomial_regression.fit(X, y)</span><br><span class="line">    y_newbig &#x3D; polynomial_regression.predict(X_new)</span><br><span class="line">    plt.plot(X_new, y_newbig, style, label&#x3D;str(degree), linewidth&#x3D;width)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, &quot;b.&quot;, linewidth&#x3D;3)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;)</span><br><span class="line">plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.axis([-3, 3, 0, 10])</span><br><span class="line">#save_fig(&quot;high_degree_polynomials_plot(高阶多项式回归)&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>纯线性回归模型学习曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">def plot_learning_curves(model, X, y):</span><br><span class="line">    X_train, X_val, y_train, y_val &#x3D; train_test_split(X, y, test_size&#x3D;0.2, random_state&#x3D;10)</span><br><span class="line">    train_errors, val_errors &#x3D; [], []</span><br><span class="line">    for m in range(1, len(X_train)):</span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        y_train_predict &#x3D; model.predict(X_train[:m])</span><br><span class="line">        y_val_predict &#x3D; model.predict(X_val)</span><br><span class="line">        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">    plt.plot(np.sqrt(train_errors), &quot;r-+&quot;, linewidth&#x3D;2, label&#x3D;&quot;train&quot;)</span><br><span class="line">    plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth&#x3D;3, label&#x3D;&quot;val&quot;)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;upper right&quot;, fontsize&#x3D;14)   # not shown in the book</span><br><span class="line">    plt.xlabel(&quot;Training set size&quot;, fontsize&#x3D;14) # not shown</span><br><span class="line">    plt.ylabel(&quot;RMSE&quot;, fontsize&#x3D;14)              # not shown</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br><span class="line">plt.axis([0, 80, 0, 3])                         # not shown in the book</span><br><span class="line">#save_fig(&quot;underfitting_learning_curves_plot(学习曲线)&quot;)   # not shown</span><br><span class="line">#plt.show()                                      # not shown</span><br></pre></td></tr></table></figure>
</li>
<li><p>多项式回归模型的学习曲线 10阶</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression &#x3D; Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;10, include_bias&#x3D;False)),</span><br><span class="line">        (&quot;lin_reg&quot;, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([0, 80, 0, 3])           # not shown</span><br><span class="line">save_fig(&quot;learning_curves_plot(多项式回归模型的学习曲线)&quot;)  # not shown</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="正则线性模型"><a href="#正则线性模型" class="headerlink" title="正则线性模型"></a>正则线性模型</h4><ol start="16">
<li><p>岭回归（也叫作吉洪诺夫正则化）是线性回归的正则化版</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 20</span><br><span class="line">X &#x3D; 3 * np.random.rand(m, 1)</span><br><span class="line">y &#x3D; 1 + 0.5 * X + np.random.randn(m, 1) &#x2F; 1.5</span><br><span class="line">X_new &#x3D; np.linspace(0, 3, 100).reshape(100, 1)</span><br><span class="line"></span><br><span class="line">def plot_model(model_class, polynomial, alphas, **model_kargs):</span><br><span class="line">    for alpha, style in zip(alphas, (&quot;b-&quot;, &quot;g--&quot;, &quot;r:&quot;)):</span><br><span class="line">        model &#x3D; model_class(alpha, **model_kargs) if alpha &gt; 0 else LinearRegression()</span><br><span class="line">        if polynomial:</span><br><span class="line">            model &#x3D; Pipeline([</span><br><span class="line">                    (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;10, include_bias&#x3D;False)),</span><br><span class="line">                    (&quot;std_scaler&quot;, StandardScaler()),</span><br><span class="line">                    (&quot;regul_reg&quot;, model),</span><br><span class="line">                ])</span><br><span class="line">        model.fit(X, y)</span><br><span class="line">        y_new_regul &#x3D; model.predict(X_new)</span><br><span class="line">        lw &#x3D; 2 if alpha &gt; 0 else 1</span><br><span class="line">        plt.plot(X_new, y_new_regul, style, linewidth&#x3D;lw, label&#x3D;r&quot;$\alpha &#x3D; &#123;&#125;$&quot;.format(alpha))</span><br><span class="line">    plt.plot(X, y, &quot;b.&quot;, linewidth&#x3D;3)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;15)</span><br><span class="line">    plt.xlabel(&quot;$x_1$&quot;, fontsize&#x3D;18)</span><br><span class="line">    plt.axis([0, 3, 0, 4])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8,4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_model(Ridge, polynomial&#x3D;False, alphas&#x3D;(0, 10, 100), random_state&#x3D;42)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_model(Ridge, polynomial&#x3D;True, alphas&#x3D;(0, 10**-5, 1), random_state&#x3D;42)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;ridge_regression_plot(岭回归)&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Scikit-Learn执行闭式解的岭回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line">ridge_reg &#x3D; Ridge(alpha&#x3D;1, solver&#x3D;&quot;cholesky&quot;, random_state&#x3D;42)</span><br><span class="line">ridge_reg.fit(X, y)</span><br><span class="line">ridge_reg.predict([[1.5]])</span><br><span class="line"></span><br><span class="line">ridge_reg &#x3D; Ridge(alpha&#x3D;1, solver&#x3D;&quot;sag&quot;, random_state&#x3D;42)</span><br><span class="line">ridge_reg.fit(X, y)</span><br><span class="line">ridge_reg.predict([[1.5]])</span><br><span class="line"></span><br><span class="line">#使用随机梯度下降</span><br><span class="line">from sklearn.linear_model import SGDRegressor</span><br><span class="line">sgd_reg &#x3D; SGDRegressor(max_iter&#x3D;50, tol&#x3D;-np.infty, penalty&#x3D;&quot;l2&quot;, random_state&#x3D;42)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.predict([[1.5]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>套索回归、Lasso回归.线性回归的另一种正则化，叫作最小绝对收缩和选择算子回归（Least Absolute Shrinkage and Selection Operator Regression，简称Lasso回归，或套索回归）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8,4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plot_model(Lasso, polynomial&#x3D;False, alphas&#x3D;(0, 0.1, 1), random_state&#x3D;42)</span><br><span class="line">plt.ylabel(&quot;$y$&quot;, rotation&#x3D;0, fontsize&#x3D;18)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_model(Lasso, polynomial&#x3D;True, alphas&#x3D;(0, 10**-7, 1), tol&#x3D;1, random_state&#x3D;42)</span><br><span class="line">#save_fig(&quot;lasso_regression_plot(套索回归Lasso回归)&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Scikit-Learn的Lasso类的小例子。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import ElasticNet</span><br><span class="line">elastic_net &#x3D; ElasticNet(alpha&#x3D;0.1, l1_ratio&#x3D;0.5, random_state&#x3D;42)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line">#print(elastic_net.predict([[1.5]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>弹性网络，使用Scikit-Learn的ElasticNet的小例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import ElasticNet</span><br><span class="line">elastic_net &#x3D; ElasticNet(alpha&#x3D;0.1, l1_ratio&#x3D;0.5, random_state&#x3D;42)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line">#print(elastic_net.predict([[1.5]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>早期停止法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import SGDRegressor</span><br><span class="line">np.random.seed(42)</span><br><span class="line">m &#x3D; 100</span><br><span class="line">X &#x3D; 6 * np.random.rand(m, 1) - 3</span><br><span class="line">y &#x3D; 2 + X + 0.5 * X**2 + np.random.randn(m, 1)</span><br><span class="line"></span><br><span class="line">X_train, X_val, y_train, y_val &#x3D; train_test_split(X[:50], y[:50].ravel(), test_size&#x3D;0.5, random_state&#x3D;10)</span><br><span class="line"></span><br><span class="line">poly_scaler &#x3D; Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree&#x3D;90, include_bias&#x3D;False)),</span><br><span class="line">        (&quot;std_scaler&quot;, StandardScaler()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">X_train_poly_scaled &#x3D; poly_scaler.fit_transform(X_train)</span><br><span class="line">X_val_poly_scaled &#x3D; poly_scaler.transform(X_val)</span><br><span class="line"></span><br><span class="line">sgd_reg &#x3D; SGDRegressor(max_iter&#x3D;1,</span><br><span class="line">                    tol&#x3D;-np.infty,</span><br><span class="line">                    penalty&#x3D;None,</span><br><span class="line">                    eta0&#x3D;0.0005,</span><br><span class="line">                    warm_start&#x3D;True,</span><br><span class="line">                    learning_rate&#x3D;&quot;constant&quot;,</span><br><span class="line">                    random_state&#x3D;42)</span><br><span class="line"></span><br><span class="line">n_epochs &#x3D; 500</span><br><span class="line">train_errors, val_errors &#x3D; [], []</span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    sgd_reg.fit(X_train_poly_scaled, y_train)</span><br><span class="line">    y_train_predict &#x3D; sgd_reg.predict(X_train_poly_scaled)</span><br><span class="line">    y_val_predict &#x3D; sgd_reg.predict(X_val_poly_scaled)</span><br><span class="line">    train_errors.append(mean_squared_error(y_train, y_train_predict))</span><br><span class="line">    val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">best_epoch &#x3D; np.argmin(val_errors)</span><br><span class="line">best_val_rmse &#x3D; np.sqrt(val_errors[best_epoch])</span><br><span class="line"></span><br><span class="line">plt.annotate(&#39;Best model&#39;,</span><br><span class="line">            xy&#x3D;(best_epoch, best_val_rmse),</span><br><span class="line">            xytext&#x3D;(best_epoch, best_val_rmse + 1),</span><br><span class="line">            ha&#x3D;&quot;center&quot;,</span><br><span class="line">            arrowprops&#x3D;dict(facecolor&#x3D;&#39;black&#39;, shrink&#x3D;0.05),</span><br><span class="line">            fontsize&#x3D;16,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">best_val_rmse -&#x3D; 0.03  # just to make the graph look better</span><br><span class="line">plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], &quot;k:&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth&#x3D;3, label&#x3D;&quot;Validation set&quot;)</span><br><span class="line">plt.plot(np.sqrt(train_errors), &quot;r--&quot;, linewidth&#x3D;2, label&#x3D;&quot;Training set&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper right&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.xlabel(&quot;Epoch&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;RMSE&quot;, fontsize&#x3D;14)</span><br><span class="line">save_fig(&quot;early_stopping_plot(早期停止法)&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Lasso回归与岭回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">t1a, t1b, t2a, t2b &#x3D; -1, 3, -1.5, 1.5</span><br><span class="line"></span><br><span class="line"># ignoring bias term</span><br><span class="line">t1s &#x3D; np.linspace(t1a, t1b, 500)</span><br><span class="line">t2s &#x3D; np.linspace(t2a, t2b, 500)</span><br><span class="line">t1, t2 &#x3D; np.meshgrid(t1s, t2s)</span><br><span class="line">T &#x3D; np.c_[t1.ravel(), t2.ravel()]</span><br><span class="line">Xr &#x3D; np.array([[-1, 1], [-0.3, -1], [1, 0.1]])</span><br><span class="line">yr &#x3D; 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]</span><br><span class="line"></span><br><span class="line">J &#x3D; (1&#x2F;len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis&#x3D;1)).reshape(t1.shape)</span><br><span class="line"></span><br><span class="line">N1 &#x3D; np.linalg.norm(T, ord&#x3D;1, axis&#x3D;1).reshape(t1.shape)</span><br><span class="line">N2 &#x3D; np.linalg.norm(T, ord&#x3D;2, axis&#x3D;1).reshape(t1.shape)</span><br><span class="line"></span><br><span class="line">t_min_idx &#x3D; np.unravel_index(np.argmin(J), J.shape)</span><br><span class="line">t1_min, t2_min &#x3D; t1[t_min_idx], t2[t_min_idx]</span><br><span class="line"></span><br><span class="line">t_init &#x3D; np.array([[0.25], [-1]])</span><br><span class="line"></span><br><span class="line">def bgd_path(theta, X, y, l1, l2, core &#x3D; 1, eta &#x3D; 0.1, n_iterations &#x3D; 50):</span><br><span class="line">    path &#x3D; [theta]</span><br><span class="line">    for iteration in range(n_iterations):</span><br><span class="line">        gradients &#x3D; core * 2&#x2F;len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + 2 * l2 * theta</span><br><span class="line"></span><br><span class="line">        theta &#x3D; theta - eta * gradients</span><br><span class="line">        path.append(theta)</span><br><span class="line">    return np.array(path)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(12, 8))</span><br><span class="line">for i, N, l1, l2, title in ((0, N1, 0.5, 0, &quot;Lasso&quot;), (1, N2, 0,  0.1, &quot;Ridge&quot;)):</span><br><span class="line">    JR &#x3D; J + l1 * N1 + l2 * N2**2</span><br><span class="line">    </span><br><span class="line">    tr_min_idx &#x3D; np.unravel_index(np.argmin(JR), JR.shape)</span><br><span class="line">    t1r_min, t2r_min &#x3D; t1[tr_min_idx], t2[tr_min_idx]</span><br><span class="line"></span><br><span class="line">    levelsJ&#x3D;(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)</span><br><span class="line">    levelsJR&#x3D;(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)</span><br><span class="line">    levelsN&#x3D;np.linspace(0, np.max(N), 10)</span><br><span class="line">    </span><br><span class="line">    path_J &#x3D; bgd_path(t_init, Xr, yr, l1&#x3D;0, l2&#x3D;0)</span><br><span class="line">    path_JR &#x3D; bgd_path(t_init, Xr, yr, l1, l2)</span><br><span class="line">    path_N &#x3D; bgd_path(t_init, Xr, yr, np.sign(l1)&#x2F;3, np.sign(l2), core&#x3D;0)</span><br><span class="line"></span><br><span class="line">    plt.subplot(221 + i * 2)</span><br><span class="line">    plt.grid(True)</span><br><span class="line">    plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.contourf(t1, t2, J, levels&#x3D;levelsJ, alpha&#x3D;0.9)</span><br><span class="line">    plt.contour(t1, t2, N, levels&#x3D;levelsN)</span><br><span class="line">    plt.plot(path_J[:, 0], path_J[:, 1], &quot;w-o&quot;)</span><br><span class="line">    plt.plot(path_N[:, 0], path_N[:, 1], &quot;y-^&quot;)</span><br><span class="line">    plt.plot(t1_min, t2_min, &quot;rs&quot;)</span><br><span class="line">    plt.title(r&quot;$\ell_&#123;&#125;$ penalty&quot;.format(i + 1), fontsize&#x3D;16)</span><br><span class="line">    plt.axis([t1a, t1b, t2a, t2b])</span><br><span class="line">    if i &#x3D;&#x3D; 1:</span><br><span class="line">        plt.xlabel(r&quot;$\theta_1$&quot;, fontsize&#x3D;20)</span><br><span class="line">    plt.ylabel(r&quot;$\theta_2$&quot;, fontsize&#x3D;20, rotation&#x3D;0)</span><br><span class="line"></span><br><span class="line">    plt.subplot(222 + i * 2)</span><br><span class="line">    plt.grid(True)</span><br><span class="line">    plt.axhline(y&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.axvline(x&#x3D;0, color&#x3D;&#39;k&#39;)</span><br><span class="line">    plt.contourf(t1, t2, JR, levels&#x3D;levelsJR, alpha&#x3D;0.9)</span><br><span class="line">    plt.plot(path_JR[:, 0], path_JR[:, 1], &quot;w-o&quot;)</span><br><span class="line">    plt.plot(t1r_min, t2r_min, &quot;rs&quot;)</span><br><span class="line">    plt.title(title, fontsize&#x3D;16)</span><br><span class="line">    plt.axis([t1a, t1b, t2a, t2b])</span><br><span class="line">    if i &#x3D;&#x3D; 1:</span><br><span class="line">        plt.xlabel(r&quot;$\theta_1$&quot;, fontsize&#x3D;20)</span><br><span class="line"></span><br><span class="line">save_fig(&quot;lasso_vs_ridge_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>逻辑回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#逻辑函数</span><br><span class="line">t &#x3D; np.linspace(-10, 10, 100)</span><br><span class="line">sig &#x3D; 1 &#x2F; (1 + np.exp(-t))</span><br><span class="line">plt.figure(figsize&#x3D;(9, 3))</span><br><span class="line">plt.plot([-10, 10], [0, 0], &quot;k-&quot;)</span><br><span class="line">plt.plot([-10, 10], [0.5, 0.5], &quot;k:&quot;)</span><br><span class="line">plt.plot([-10, 10], [1, 1], &quot;k:&quot;)</span><br><span class="line">plt.plot([0, 0], [-1.1, 1.1], &quot;k-&quot;)</span><br><span class="line">plt.plot(t, sig, &quot;b-&quot;, linewidth&#x3D;2, label&#x3D;r&quot;$\sigma(t) &#x3D; \frac&#123;1&#125;&#123;1 + e^&#123;-t&#125;&#125;$&quot;)</span><br><span class="line">plt.xlabel(&quot;t&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;upper left&quot;, fontsize&#x3D;20)</span><br><span class="line">plt.axis([-10, 10, -0.1, 1.1])</span><br><span class="line">save_fig(&quot;logistic_function_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>决策边界</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#创建一个分类器来检测Virginica鸢尾花。</span><br><span class="line">from sklearn import datasets</span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">list(iris.keys())</span><br><span class="line">#print(list(iris.keys()))</span><br><span class="line">#print(iris.DESCR)</span><br><span class="line"></span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, 3:]  # petal width</span><br><span class="line">y &#x3D; (iris[&quot;target&quot;] &#x3D;&#x3D; 2).astype(np.int)  # 1 if Iris-Virginica, else 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练逻辑回归模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">log_reg &#x3D; LogisticRegression(solver&#x3D;&quot;liblinear&quot;, random_state&#x3D;42)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">#精简版</span><br><span class="line">X_new &#x3D; np.linspace(0, 3, 1000).reshape(-1, 1)</span><br><span class="line">y_proba &#x3D; log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth&#x3D;2, label&#x3D;&quot;Not Iris-Virginica&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">#完整版</span><br><span class="line">X_new &#x3D; np.linspace(0, 3, 1000).reshape(-1, 1)</span><br><span class="line">y_proba &#x3D; log_reg.predict_proba(X_new)</span><br><span class="line">decision_boundary &#x3D; X_new[y_proba[:, 1] &gt;&#x3D; 0.5][0]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8, 3))</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;0], y[y&#x3D;&#x3D;0], &quot;bs&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;1], y[y&#x3D;&#x3D;1], &quot;g^&quot;)</span><br><span class="line">plt.plot([decision_boundary, decision_boundary], [-1, 2], &quot;k:&quot;, linewidth&#x3D;2)</span><br><span class="line">plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth&#x3D;2, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth&#x3D;2, label&#x3D;&quot;Not Iris-Virginica&quot;)</span><br><span class="line">plt.text(decision_boundary+0.02, 0.15, &quot;Decision  boundary&quot;, fontsize&#x3D;14, color&#x3D;&quot;k&quot;, ha&#x3D;&quot;center&quot;)</span><br><span class="line">plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width&#x3D;0.05, head_length&#x3D;0.1, fc&#x3D;&#39;b&#39;, ec&#x3D;&#39;b&#39;)</span><br><span class="line">plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width&#x3D;0.05, head_length&#x3D;0.1, fc&#x3D;&#39;g&#39;, ec&#x3D;&#39;g&#39;)</span><br><span class="line">plt.xlabel(&quot;Petal width (cm)&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Probability&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.legend(loc&#x3D;&quot;center left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 3, -0.02, 1.02])</span><br><span class="line">#save_fig(&quot;logistic_regression_plot(估算概率和决策边界)&quot;)</span><br><span class="line">#plt.show()</span><br><span class="line">print(decision_boundary)</span><br><span class="line">print(log_reg.predict([[1.7], [1.5]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Softmax回归 多元逻辑回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y &#x3D; (iris[&quot;target&quot;] &#x3D;&#x3D; 2).astype(np.int)</span><br><span class="line"></span><br><span class="line">log_reg &#x3D; LogisticRegression(solver&#x3D;&quot;liblinear&quot;, C&#x3D;10**10, random_state&#x3D;42)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">x0, x1 &#x3D; np.meshgrid(</span><br><span class="line">        np.linspace(2.9, 7, 500).reshape(-1, 1),</span><br><span class="line">        np.linspace(0.8, 2.7, 200).reshape(-1, 1),</span><br><span class="line">    )</span><br><span class="line">X_new &#x3D; np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line">y_proba &#x3D; log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(10, 4))</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;0, 0], X[y&#x3D;&#x3D;0, 1], &quot;bs&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;1, 0], X[y&#x3D;&#x3D;1, 1], &quot;g^&quot;)</span><br><span class="line"></span><br><span class="line">zz &#x3D; y_proba[:, 1].reshape(x0.shape)</span><br><span class="line">contour &#x3D; plt.contour(x0, x1, zz, cmap&#x3D;plt.cm.brg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">left_right &#x3D; np.array([2.9, 7])</span><br><span class="line">boundary &#x3D; -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) &#x2F; log_reg.coef_[0][1]</span><br><span class="line"></span><br><span class="line">plt.clabel(contour, inline&#x3D;1, fontsize&#x3D;12)</span><br><span class="line">plt.plot(left_right, boundary, &quot;k--&quot;, linewidth&#x3D;3)</span><br><span class="line">plt.text(3.5, 1.5, &quot;Not Iris-Virginica&quot;, fontsize&#x3D;14, color&#x3D;&quot;b&quot;, ha&#x3D;&quot;center&quot;)</span><br><span class="line">plt.text(6.5, 2.3, &quot;Iris-Virginica&quot;, fontsize&#x3D;14, color&#x3D;&quot;g&quot;, ha&#x3D;&quot;center&quot;)</span><br><span class="line">plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Petal width&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([2.9, 7, 0.8, 2.7])</span><br><span class="line">save_fig(&quot;logistic_regression_contour_plot&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">X &#x3D; iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y &#x3D; iris[&quot;target&quot;]</span><br><span class="line"></span><br><span class="line">softmax_reg &#x3D; LogisticRegression(multi_class&#x3D;&quot;multinomial&quot;,solver&#x3D;&quot;lbfgs&quot;, C&#x3D;10, random_state&#x3D;42)</span><br><span class="line">softmax_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">x0, x1 &#x3D; np.meshgrid(</span><br><span class="line">        np.linspace(0, 8, 500).reshape(-1, 1),</span><br><span class="line">        np.linspace(0, 3.5, 200).reshape(-1, 1),</span><br><span class="line">    )</span><br><span class="line">X_new &#x3D; np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line">y_proba &#x3D; softmax_reg.predict_proba(X_new)</span><br><span class="line">y_predict &#x3D; softmax_reg.predict(X_new)</span><br><span class="line"></span><br><span class="line">zz1 &#x3D; y_proba[:, 1].reshape(x0.shape)</span><br><span class="line">zz &#x3D; y_predict.reshape(x0.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(10, 4))</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;2, 0], X[y&#x3D;&#x3D;2, 1], &quot;g^&quot;, label&#x3D;&quot;Iris-Virginica&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;1, 0], X[y&#x3D;&#x3D;1, 1], &quot;bs&quot;, label&#x3D;&quot;Iris-Versicolor&quot;)</span><br><span class="line">plt.plot(X[y&#x3D;&#x3D;0, 0], X[y&#x3D;&#x3D;0, 1], &quot;yo&quot;, label&#x3D;&quot;Iris-Setosa&quot;)</span><br><span class="line"></span><br><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line">custom_cmap &#x3D; ListedColormap([&#39;#fafab0&#39;,&#39;#9898ff&#39;,&#39;#a0faa0&#39;])</span><br><span class="line"></span><br><span class="line">plt.contourf(x0, x1, zz, cmap&#x3D;custom_cmap)</span><br><span class="line">contour &#x3D; plt.contour(x0, x1, zz1, cmap&#x3D;plt.cm.brg)</span><br><span class="line">plt.clabel(contour, inline&#x3D;1, fontsize&#x3D;12)</span><br><span class="line">plt.xlabel(&quot;Petal length&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.ylabel(&quot;Petal width&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.legend(loc&#x3D;&quot;center left&quot;, fontsize&#x3D;14)</span><br><span class="line">plt.axis([0, 7, 0, 3.5])</span><br><span class="line">save_fig(&quot;softmax_regression_contour_plot&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(softmax_reg.predict([[5, 2]]))</span><br><span class="line">print(softmax_reg.predict_proba([[5, 2]]))</span><br></pre></td></tr></table></figure>



















</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-python爬取网页图片" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/24/python%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/" class="article-date">
      <time datetime="2020-03-24T12:35:59.000Z" itemprop="datePublished">2020-03-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/24/python%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/">python爬取网页图片</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>完整代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import re</span><br><span class="line">import urllib</span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_html(url):</span><br><span class="line">    page &#x3D; urllib.request.urlopen(url)</span><br><span class="line">    html_a &#x3D; page.read()</span><br><span class="line">    return html_a.decode(&#39;utf-8&#39;)</span><br><span class="line"></span><br><span class="line">def get_img(url):</span><br><span class="line">    x &#x3D; 1        # 声明一个变量赋值</span><br><span class="line">    html_b &#x3D; get_html(url)</span><br><span class="line">    reg &#x3D; r&#39;https:&#x2F;&#x2F;[^\s]*?\.jpg&#39;</span><br><span class="line">    imgre &#x3D; re.compile(reg)  # 转换成一个正则对象</span><br><span class="line">    imglist &#x3D; imgre.findall(html_b)  # 表示在整个网页过滤出所有图片的地址，放在imgList中</span><br><span class="line">    path &#x3D; os.getcwd() + os.sep + &#39;image&#39;  # 设置图片的保存地址</span><br><span class="line">    if not os.path.isdir(path):</span><br><span class="line">        os.makedirs(path)  # 判断没有此路径则创建</span><br><span class="line">    paths &#x3D; path + &#39;\\&#39;  # 保存在test路径下</span><br><span class="line">    for imgurl in imglist:</span><br><span class="line">        urllib.request.urlretrieve(imgurl, &#39;&#123;0&#125;&#123;1&#125;.jpg&#39;.format(paths, x))  # 打开imgList,下载图片到本地</span><br><span class="line">        print(&#39;开始下载第%s张图片&#39;%x)</span><br><span class="line">        x &#x3D; x + 1</span><br><span class="line"></span><br><span class="line">url &#x3D; input(&quot;请输入网址&quot;)</span><br><span class="line">#url &#x3D; </span><br><span class="line">get_img(url)</span><br></pre></td></tr></table></figure>





      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-第3章分类" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/18/%E7%AC%AC3%E7%AB%A0%E5%88%86%E7%B1%BB/" class="article-date">
      <time datetime="2020-03-18T05:29:01.000Z" itemprop="datePublished">2020-03-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/18/%E7%AC%AC3%E7%AB%A0%E5%88%86%E7%B1%BB/">第3章分类</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考：作者的<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/tree/master/" target="_blank" rel="noopener">Jupyter Notebook</a><br><a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/03_classification.ipynb" target="_blank" rel="noopener">Chapter 3 – Classification</a></p>
<ol>
<li><p>获取MNIST数据集的代码：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def sort_by_target(mnist):</span><br><span class="line">    reorder_train &#x3D; np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]</span><br><span class="line">    reorder_test &#x3D; np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]</span><br><span class="line">    mnist.data[:60000] &#x3D; mnist.data[reorder_train]</span><br><span class="line">    mnist.target[:60000] &#x3D; mnist.target[reorder_train]</span><br><span class="line">    mnist.data[60000:] &#x3D; mnist.data[reorder_test + 60000]</span><br><span class="line">    mnist.target[60000:] &#x3D; mnist.target[reorder_test + 60000]</span><br><span class="line">from sklearn.datasets import fetch_openml</span><br><span class="line">mnist &#x3D; fetch_openml(&#39;mnist_784&#39;, version&#x3D;1, cache&#x3D;True)</span><br><span class="line">mnist.target &#x3D; mnist.target.astype(np.int8) # fetch_openml() returns targets as strings</span><br><span class="line">sort_by_target(mnist) # fetch_openml() returns an unsorted dataset</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看这些数组</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#print(mnist[&quot;data&quot;], mnist[&quot;target&quot;])</span><br><span class="line">#print(mnist.data.shape)</span><br><span class="line">X, y &#x3D; mnist[&quot;data&quot;], mnist[&quot;target&quot;]</span><br><span class="line">#print(X.shape)</span><br><span class="line">#print(y.shape)</span><br><span class="line"></span><br><span class="line">some_digit &#x3D; X[36000]</span><br><span class="line">some_digit_image &#x3D; some_digit.reshape(28, 28)</span><br><span class="line">plt.imshow(some_digit_image, cmap &#x3D; mpl.cm.binary,</span><br><span class="line">        interpolation&#x3D;&quot;nearest&quot;)</span><br><span class="line">plt.axis(&quot;off&quot;)</span><br><span class="line">#plt.show()</span><br><span class="line">#print(y[36000])</span><br></pre></td></tr></table></figure>
</li>
<li><p>MNIST数据集中的部分数字图像</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def plot_digits(instances, images_per_row&#x3D;10, **options):</span><br><span class="line">    size &#x3D; 28</span><br><span class="line">    images_per_row &#x3D; min(len(instances), images_per_row)</span><br><span class="line">    images &#x3D; [instance.reshape(size,size) for instance in instances]</span><br><span class="line">    n_rows &#x3D; (len(instances) - 1) &#x2F;&#x2F; images_per_row + 1</span><br><span class="line">    row_images &#x3D; []</span><br><span class="line">    n_empty &#x3D; n_rows * images_per_row - len(instances)</span><br><span class="line">    images.append(np.zeros((size, size * n_empty)))</span><br><span class="line">    for row in range(n_rows):</span><br><span class="line">        rimages &#x3D; images[row * images_per_row : (row + 1) * images_per_row]</span><br><span class="line">        row_images.append(np.concatenate(rimages, axis&#x3D;1))</span><br><span class="line">    image &#x3D; np.concatenate(row_images, axis&#x3D;0)</span><br><span class="line">    plt.imshow(image, cmap &#x3D; mpl.cm.binary, **options)</span><br><span class="line">    plt.axis(&quot;off&quot;)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(9,9))</span><br><span class="line">example_images &#x3D; np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]</span><br><span class="line">plot_digits(example_images, images_per_row&#x3D;10)</span><br><span class="line">#save_fig(&quot;more_digits_plot&quot;)</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>给数据集洗牌</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test &#x3D; X[:60000], X[60000:], y[:60000], y[60000:]</span><br><span class="line">shuffle_index &#x3D; np.random.permutation(60000)</span><br><span class="line">X_train, y_train &#x3D; X_train[shuffle_index], y_train[shuffle_index]</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练一个二元分类器，为此分类任务创建目标向量：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_5 &#x3D; (y_train &#x3D;&#x3D; 5)  # True for all 5s, False for all other digits.</span><br><span class="line">y_test_5 &#x3D; (y_test &#x3D;&#x3D; 5)</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个SGDClassifier(随机梯度下降分类器)并在整个训练集上进行训练：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import SGDClassifier</span><br><span class="line">sgd_clf &#x3D; SGDClassifier(max_iter&#x3D;5, tol&#x3D;-np.infty, random_state&#x3D;42)   #random_state&#x3D;42</span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br><span class="line">#print(sgd_clf.fit(X_train, y_train_5))</span><br><span class="line">#现在可以用它来检测数字5的图像了：</span><br><span class="line">sgd_clf.predict([some_digit])</span><br><span class="line">#print(sgd_clf.predict([some_digit]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>交叉验证</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train_5, cv&#x3D;3, scoring&#x3D;&quot;accuracy&quot;)</span><br><span class="line">print(cross_val_score(sgd_clf, X_train, y_train_5, cv&#x3D;3, scoring&#x3D;&quot;accuracy&quot;))</span><br><span class="line"></span><br><span class="line">#下面这段代码与前面的cross_val_score（）大致相同，并打印出相同的结果：</span><br><span class="line">from sklearn.model_selection import StratifiedKFold</span><br><span class="line">from sklearn.base import clone</span><br><span class="line">skfolds &#x3D; StratifiedKFold(n_splits&#x3D;3, random_state&#x3D;42)   #random_state&#x3D;42</span><br><span class="line">for train_index, test_index in skfolds.split(X_train, y_train_5):</span><br><span class="line">    clone_clf &#x3D; clone(sgd_clf)</span><br><span class="line">    X_train_folds &#x3D; X_train[train_index]</span><br><span class="line">    y_train_folds &#x3D; (y_train_5[train_index])</span><br><span class="line">    X_test_fold &#x3D; X_train[test_index]</span><br><span class="line">    y_test_fold &#x3D; (y_train_5[test_index])</span><br><span class="line"></span><br><span class="line">    clone_clf.fit(X_train_folds, y_train_folds)</span><br><span class="line">    y_pred &#x3D; clone_clf.predict(X_test_fold)</span><br><span class="line">    n_correct &#x3D; sum(y_pred &#x3D;&#x3D; y_test_fold)</span><br><span class="line">    print(n_correct &#x2F; len(y_pred))</span><br></pre></td></tr></table></figure>
</li>
<li><p>一个蠢笨的分类器(不是我说的)，它将每张图都分类成“非5”：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator</span><br><span class="line">class Never5Classifier(BaseEstimator):</span><br><span class="line">    def fit(self, X, y&#x3D;None):</span><br><span class="line">        pass</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        return np.zeros((len(X), 1), dtype&#x3D;bool)</span><br><span class="line">#准确度</span><br><span class="line">never_5_clf &#x3D; Never5Classifier()</span><br><span class="line">print(cross_val_score(never_5_clf, X_train, y_train_5, cv&#x3D;3, scoring&#x3D;&quot;accuracy&quot;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>混淆矩阵:评估分类器性能的更好方法是混淆矩阵。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_predict</span><br><span class="line">y_train_pred &#x3D; cross_val_predict(sgd_clf, X_train, y_train_5, cv&#x3D;3)</span><br><span class="line">#cross_val_predict（）函数同样执行K-fold交叉验证，但返回的不是评估分数，而是每个折叠的预测。这意味着对于每个实例都可以得到一个干净的预测</span><br><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">#confusion_matrix(y_train_5, y_train_pred)</span><br><span class="line">#print(confusion_matrix(y_train_5, y_train_pred))</span><br><span class="line">y_train_perfect_predictions &#x3D; y_train_5</span><br><span class="line">#print(confusion_matrix(y_train_5, y_train_perfect_predictions))</span><br></pre></td></tr></table></figure>
</li>
<li><p>精度和召回率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#精度&#x3D;TP&#x2F;(TP+FP)：TP是真正类的数量，FP是假正类的数量。</span><br><span class="line">#召回率&#x3D;TP&#x2F;(TP+FN)：FN是假负类的数量。</span><br><span class="line">from sklearn.metrics import precision_score, recall_score</span><br><span class="line">print(precision_score(y_train_5, y_train_pred))  #精度4344 &#x2F; (4344 + 1307)</span><br><span class="line">print(recall_score(y_train_5, y_train_pred))  #召回率4344 &#x2F; (4344 + 1077)</span><br><span class="line"></span><br><span class="line">#F1分数：F1&#x3D;2&#x2F;（1&#x2F;精度+1&#x2F;召回率）&#x3D;TP&#x2F;(TP+(FN+FP)&#x2F;2)</span><br><span class="line">from sklearn.metrics import f1_score</span><br><span class="line">print(f1_score(y_train_5, y_train_pred))</span><br></pre></td></tr></table></figure>
</li>
<li><p>精度/召回率权衡：阈值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">y_scores &#x3D; sgd_clf.decision_function([some_digit])</span><br><span class="line">print(y_scores)</span><br><span class="line">threshold &#x3D; 0</span><br><span class="line">y_some_digit_pred &#x3D; (y_scores &gt; threshold)</span><br><span class="line">print(y_some_digit_pred)</span><br><span class="line">#提高阈值</span><br><span class="line">threshold &#x3D; 200000</span><br><span class="line">y_some_digit_pred_a &#x3D; (y_scores &gt; threshold)</span><br><span class="line">print(y_some_digit_pred_a)</span><br></pre></td></tr></table></figure>
</li>
<li><p>决定使用什么阈值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#获取训练集中所有实例的分数</span><br><span class="line">y_scores &#x3D; cross_val_predict(sgd_clf, X_train, y_train_5, cv&#x3D;3, method&#x3D;&quot;decision_function&quot;)</span><br><span class="line">#计算所有可能的阈值的精度和召回率</span><br><span class="line">from sklearn.metrics import precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds &#x3D; precision_recall_curve(y_train_5, y_scores)</span><br><span class="line">#使用Matplotlib绘制精度和召回率相对于阈值的函数图</span><br><span class="line">def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):</span><br><span class="line">    plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label&#x3D;&quot;Precision&quot;)</span><br><span class="line">    plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label&#x3D;&quot;Recall&quot;)</span><br><span class="line">    plt.xlabel(&quot;Threshold&quot;)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;upper left&quot;)</span><br><span class="line">    plt.ylim([0, 1])</span><br><span class="line">plt.figure(figsize&#x3D;(8, 4))</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.xlim([-700000, 700000])</span><br><span class="line">plt.show()</span><br><span class="line">#print((y_train_pred &#x3D;&#x3D; (y_scores &gt; 0)).all())</span><br><span class="line">y_train_pred_90 &#x3D; (y_scores &gt; 70000)</span><br><span class="line">from sklearn.metrics import precision_score, recall_score</span><br><span class="line">print(precision_score(y_train_5, y_train_pred_90)) #精度</span><br><span class="line">print(recall_score(y_train_5, y_train_pred_90)) #召回率</span><br></pre></td></tr></table></figure>
</li>
<li><p>精度和召回率的函数图PR</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def plot_precision_vs_recall(precisions, recalls):</span><br><span class="line">    plt.plot(recalls, precisions, &quot;b-&quot;, linewidth&#x3D;2)</span><br><span class="line">    plt.xlabel(&quot;Recall&quot;, fontsize&#x3D;16)</span><br><span class="line">    plt.ylabel(&quot;Precision&quot;, fontsize&#x3D;16)</span><br><span class="line">    plt.axis([0, 1, 0, 1])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8, 6))</span><br><span class="line">plot_precision_vs_recall(precisions, recalls)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>ROC曲线（受试者工作特征曲线）：真正类率和假正类率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import roc_curve</span><br><span class="line">fpr, tpr, thresholds &#x3D; roc_curve(y_train_5, y_scores)</span><br><span class="line">def plot_roc_curve(fpr, tpr, label&#x3D;None):</span><br><span class="line">    plt.plot(fpr, tpr, linewidth&#x3D;2, label&#x3D;label)</span><br><span class="line">    plt.plot([0, 1], [0, 1], &#39;k--&#39;)</span><br><span class="line">    plt.axis([0, 1, 0, 1])</span><br><span class="line">    plt.xlabel(&#39;False Positive Rate&#39;, fontsize&#x3D;16)</span><br><span class="line">    plt.ylabel(&#39;True Positive Rate&#39;, fontsize&#x3D;16)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">plt.figure(figsize&#x3D;(8, 6))</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">plt.show()</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">print(roc_auc_score(y_train_5, y_scores))</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练一个RandomForestClassifier分类器，并比较它和SGDClassifier分类器的ROC曲线和ROC AUC分数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">forest_clf &#x3D; RandomForestClassifier(n_estimators&#x3D;10, random_state&#x3D;42)</span><br><span class="line">y_probas_forest &#x3D; cross_val_predict(forest_clf, X_train, y_train_5, cv&#x3D;3, method&#x3D;&quot;predict_proba&quot;)</span><br><span class="line">y_scores_forest &#x3D; y_probas_forest[:, 1] # score &#x3D; proba of positive class</span><br><span class="line">fpr_forest, tpr_forest, thresholds_forest &#x3D; roc_curve(y_train_5,y_scores_forest)</span><br><span class="line">plt.figure(figsize&#x3D;(8, 6))</span><br><span class="line">plt.plot(fpr, tpr, &quot;b:&quot;, linewidth&#x3D;2, label&#x3D;&quot;SGD&quot;)</span><br><span class="line">plot_roc_curve(fpr_forest, tpr_forest, &quot;Random Forest&quot;)</span><br><span class="line">plt.legend(loc&#x3D;&quot;lower right&quot;, fontsize&#x3D;16)</span><br><span class="line">plt.show()</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">print(roc_auc_score(y_train_5, y_scores_forest))</span><br></pre></td></tr></table></figure>
</li>
<li><p>多类别分类器，用SGDClassifier试试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#用SGDClassifier试试：</span><br><span class="line">sgd_clf.fit(X_train, y_train)</span><br><span class="line">sgd_clf.predict([some_digit])</span><br><span class="line">#print(sgd_clf.predict([some_digit]))</span><br><span class="line">some_digit_scores &#x3D; sgd_clf.decision_function([some_digit])</span><br><span class="line">#print(some_digit_scores)</span><br><span class="line">#print(np.argmax(some_digit_scores))</span><br><span class="line">#print(sgd_clf.classes_)</span><br><span class="line">#print(sgd_clf.classes_[5])</span><br><span class="line"></span><br><span class="line">#下面这段代码使用OvO策略，基于SGDClassifier创建了一个多类别分类器：</span><br><span class="line">from sklearn.multiclass import OneVsOneClassifier</span><br><span class="line">ovo_clf &#x3D; OneVsOneClassifier(SGDClassifier(max_iter&#x3D;5, tol&#x3D;-np.infty, random_state&#x3D;42))</span><br><span class="line">ovo_clf.fit(X_train, y_train)</span><br><span class="line">ovo_clf.predict([some_digit])</span><br><span class="line">len(ovo_clf.estimators_)</span><br><span class="line">#print(ovo_clf.predict([some_digit]))</span><br><span class="line">#print(len(ovo_clf.estimators_))</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练RandomForestClassifier</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">forest_clf.fit(X_train, y_train)</span><br><span class="line">#print(forest_clf.predict([some_digit]))</span><br><span class="line">#print(forest_clf.predict_proba([some_digit]))  #概率列表</span><br><span class="line">#print(cross_val_score(sgd_clf, X_train, y_train, cv&#x3D;3, scoring&#x3D;&quot;accuracy&quot;))  #准确率</span><br><span class="line">#将输入进行简单缩放</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">scaler &#x3D; StandardScaler()</span><br><span class="line">X_train_scaled &#x3D; scaler.fit_transform(X_train.astype(np.float64))</span><br><span class="line">#print(cross_val_score(sgd_clf, X_train_scaled, y_train, cv&#x3D;3, scoring&#x3D;&quot;accuracy&quot;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Matplotlib的matshow（）函数来查看混淆矩阵</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_train_pred &#x3D; cross_val_predict(sgd_clf, X_train_scaled, y_train, cv&#x3D;3)</span><br><span class="line">conf_mx &#x3D; confusion_matrix(y_train, y_train_pred)</span><br><span class="line">#print(conf_mx)</span><br><span class="line">#使用Matplotlib的matshow（）函数来查看混淆矩阵的图像表示</span><br><span class="line">#plt.matshow(conf_mx, cmap&#x3D;plt.cm.gray)</span><br><span class="line">#save_fig(&quot;confusion_matrix_plot&quot;, tight_layout&#x3D;False)</span><br></pre></td></tr></table></figure>
</li>
<li><p>你需要将混淆矩阵中的每个值除以相应类别中的图片数量，这样你比较的就是错误率而不是错误的绝对值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">row_sums &#x3D; conf_mx.sum(axis&#x3D;1, keepdims&#x3D;True)</span><br><span class="line">norm_conf_mx &#x3D; conf_mx &#x2F; row_sums</span><br><span class="line">#用0填充对角线，只保留错误，重新绘制结果：</span><br><span class="line">np.fill_diagonal(norm_conf_mx, 0)</span><br><span class="line">plt.matshow(norm_conf_mx, cmap&#x3D;plt.cm.gray)</span><br><span class="line">#save_fig(&quot;confusion_matrix_errors_plot&quot;, tight_layout&#x3D;False)</span><br></pre></td></tr></table></figure>
</li>
<li><p>看看数字3和数字5的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cl_a, cl_b &#x3D; 3, 5</span><br><span class="line">X_aa &#x3D; X_train[(y_train &#x3D;&#x3D; cl_a) &amp; (y_train_pred &#x3D;&#x3D; cl_a)]</span><br><span class="line">X_ab &#x3D; X_train[(y_train &#x3D;&#x3D; cl_a) &amp; (y_train_pred &#x3D;&#x3D; cl_b)]</span><br><span class="line">X_ba &#x3D; X_train[(y_train &#x3D;&#x3D; cl_b) &amp; (y_train_pred &#x3D;&#x3D; cl_a)]</span><br><span class="line">X_bb &#x3D; X_train[(y_train &#x3D;&#x3D; cl_b) &amp; (y_train_pred &#x3D;&#x3D; cl_b)]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8,8))</span><br><span class="line">plt.subplot(221); plot_digits(X_aa[:25], images_per_row&#x3D;5)</span><br><span class="line">plt.subplot(222); plot_digits(X_ab[:25], images_per_row&#x3D;5)</span><br><span class="line">plt.subplot(223); plot_digits(X_ba[:25], images_per_row&#x3D;5)</span><br><span class="line">plt.subplot(224); plot_digits(X_bb[:25], images_per_row&#x3D;5)</span><br><span class="line">#save_fig(&quot;error_analysis_digits_plot&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>多标签分类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#这段代码会创建一个y_multilabel数组，其中包含两个数字图片的目标标签：第一个表示数字是否是大数（7、8、9），第二个表示是否为奇数。</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">y_train_large &#x3D; (y_train &gt;&#x3D; 7)</span><br><span class="line">y_train_odd &#x3D; (y_train % 2 &#x3D;&#x3D; 1)</span><br><span class="line">y_multilabel &#x3D; np.c_[y_train_large, y_train_odd]</span><br><span class="line"></span><br><span class="line">knn_clf &#x3D; KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_multilabel)</span><br><span class="line">#print(knn_clf.fit(X_train, y_multilabel))</span><br><span class="line">#下一行创建一个KNeighborsClassifier实例（它支持多标签分类，不是所有的分类器都支持），然后使用多个目标数组对它进行</span><br><span class="line">#训练。现在用它做一个预测，注意它输出的两个标签：</span><br><span class="line">knn_clf.predict([some_digit])    #数字5确实不大（False），为奇数（True）。</span><br><span class="line">#print(knn_clf.predict([some_digit]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面这段代码计算所有标签的平均F1分数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import f1_score</span><br><span class="line">y_train_knn_pred &#x3D; cross_val_predict(knn_clf, X_train, y_multilabel, cv&#x3D;3, n_jobs&#x3D;-1)</span><br><span class="line">f1_score(y_multilabel, y_train_knn_pred, average&#x3D;&quot;macro&quot;)</span><br><span class="line">#print(f1_score(y_multilabel, y_train_knn_pred, average&#x3D;&quot;macro&quot;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>多输出分类(多输出-多类别分类)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#还先从创建训练集和测试集开始，使用NumPy的randint（）函数</span><br><span class="line">#为MNIST图片的像素强度增加噪声。目标是将图片还原为原始图片：</span><br><span class="line">noise &#x3D; np.random.randint(0, 100, (len(X_train), 784))</span><br><span class="line">X_train_mod &#x3D; X_train + noise</span><br><span class="line">noise &#x3D; np.random.randint(0, 100, (len(X_test), 784))</span><br><span class="line">X_test_mod &#x3D; X_test + noise</span><br><span class="line">y_train_mod &#x3D; X_train</span><br><span class="line">y_test_mod &#x3D; X_test</span><br><span class="line"></span><br><span class="line">some_index &#x3D; 5500</span><br><span class="line">#plt.subplot(121); plot_digit(X_test_mod[some_index])</span><br><span class="line">#plt.subplot(122); plot_digit(y_test_mod[some_index])</span><br><span class="line">#save_fig(&quot;noisy_digit_example_plot&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>清洗这张图片：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_mod, y_train_mod)</span><br><span class="line">clean_digit &#x3D; knn_clf.predict([X_test_mod[some_index]])</span><br><span class="line">plot_digit(clean_digit)</span><br><span class="line">save_fig(&quot;cleaned_digit_example_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-第2章端到端的机器学习项目" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/11/%E7%AC%AC2%E7%AB%A0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/" class="article-date">
      <time datetime="2020-03-10T17:14:42.000Z" itemprop="datePublished">2020-03-11</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/11/%E7%AC%AC2%E7%AB%A0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/">第2章端到端的机器学习项目</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参考：作者的<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/tree/master/" target="_blank" rel="noopener">Jupyter Notebook</a><br><a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb" target="_blank" rel="noopener">Chapter 2 – End-to-end Machine Learning project</a></p>
<ol>
<li><p>下载数据</p>
<ul>
<li>打开vscode，建立新的python文件，输入以下代码，下载housing.tgz文件，并将housing.csv解压到这个目录<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import tarfile</span><br><span class="line">from six.moves import urllib</span><br><span class="line"></span><br><span class="line">download_root &#x3D; &quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;ageron&#x2F;handson-ml&#x2F;master&#x2F;&quot;</span><br><span class="line">HOUSING_PATH &#x3D; &quot;datasets&#x2F;housing&quot;</span><br><span class="line">HOUSING_URL &#x3D; download_root + HOUSING_PATH + &quot;&#x2F;housing.tgz&quot;</span><br><span class="line"></span><br><span class="line">def fetch_housing_data(housing_url&#x3D;HOUSING_URL,housing_path&#x3D;HOUSING_PATH):</span><br><span class="line">    if not os.path.isdir(housing_path):</span><br><span class="line">        os.makedirs(housing_path)</span><br><span class="line">    tgz_path &#x3D; os.path.join(housing_path, &quot;housing.tgz&quot;)</span><br><span class="line">    urllib.request.urlretrieve(housing_url, tgz_path)</span><br><span class="line">    housing_tgz &#x3D; tarfile.open(tgz_path)</span><br><span class="line">    housing_tgz.extractall(path&#x3D;housing_path)</span><br><span class="line">    housing_tgz.close()</span><br><span class="line"></span><br><span class="line">fetch_housing_data()</span><br></pre></td></tr></table></figure>
下载后可将函数注释</li>
</ul>
</li>
<li><p>快速查看数据结构</p>
<ul>
<li><p>使用pandas加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mport pandas as pd</span><br><span class="line">def load_housing_data(housing_path&#x3D;HOUSING_PATH):</span><br><span class="line">  csv_path &#x3D; os.path.join(housing_path, &quot;housing.csv&quot;)</span><br><span class="line">  return pd.read_csv(csv_path)</span><br></pre></td></tr></table></figure>
<p>函数返回一个包含所有数据的Pandas DataFrame对象</p>
</li>
<li><p>调用DataFrames的head()方法查看前5行数据（由于使用的是vscode所以会和书里有所不同）,查看完可注释</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">housing &#x3D; load_housing_data()</span><br><span class="line">print(housing.head())</span><br></pre></td></tr></table></figure>
<p>总共有10个属性</p>
</li>
<li><p>通过info（）方法可以快速获取数据集的简单描述，特别是总行数、每个属性的类型和非空值的数量<br><code>print(housing.info())</code></p>
</li>
<li><p>使用value_counts（）方法查看有多少种分类存在，每种类别下分别有多少个区域<br><code>print(housing[&quot;ocean_proximity&quot;].value_counts())</code></p>
</li>
<li><p>通过describe（）方法可以显示数值属性的摘要<br><code>print(housing.describe())</code></p>
</li>
<li><p>在整个数据集上调用hist（）方法，绘制每个属性的直方图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">housing.hist(bins&#x3D;50, figsize&#x3D;(50,15))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>创建测试集</p>
<ul>
<li>理论上，创建测试集非常简单：只需要随机选择一些实例，通常是数据集的20%，然后将它们放在一边：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def split_train_test(data, test_ratio):</span><br><span class="line">    shuffled_indices &#x3D; np.random.permutation(len(data))</span><br><span class="line">    test_set_size &#x3D; int(len(data) * test_ratio)</span><br><span class="line">    test_indices &#x3D; shuffled_indices[:test_set_size]</span><br><span class="line">    train_indices &#x3D; shuffled_indices[test_set_size:]</span><br><span class="line">    return data.iloc[train_indices], data.iloc[test_indices]</span><br><span class="line"></span><br><span class="line">train_set, test_set &#x3D; split_train_test(housing, 0.2)</span><br><span class="line">print(len(train_set), &quot;train +&quot;, len(test_set), &quot;test&quot;)</span><br></pre></td></tr></table></figure></li>
<li>但这并不完美：如果你再运行一遍，它又会产生一个不同的数据集！这样下去，你（或者是你的机器学习算法）将会看到整个完整的数据集，而这正是创建测试集时需要避免的。常见的解决办法是每个实例都使用一个标识符（identifier）来决定是否进入测试集（假定每个实例都有一个唯一且不变的标识符）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line">def test_set_check(identifier,test_ratio, hash):</span><br><span class="line">    return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio</span><br><span class="line"></span><br><span class="line">def split_train_test_by_id(data, test_ratio, id_column, hash&#x3D;hashlib.md5):</span><br><span class="line">    ids &#x3D; data[id_column]</span><br><span class="line">    in_test_set &#x3D; ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))</span><br><span class="line">    return data.loc[~in_test_set], data.loc[in_test_set]</span><br><span class="line"></span><br><span class="line">#housing_with_id &#x3D; housing.reset_index()</span><br><span class="line">#housing_with_id[&quot;id&quot;] &#x3D; housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;]</span><br><span class="line">#train_set, test_set &#x3D; split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;)</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">train_set, test_set &#x3D; train_test_split(housing, test_size&#x3D;0.2, random&#x3D;42)</span><br></pre></td></tr></table></figure></li>
<li>分层抽样<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">housing[&quot;income_cat&quot;] &#x3D; np.ceil(housing[&quot;median_income&quot;] &#x2F; 1.5)</span><br><span class="line">housing[&quot;income_cat&quot;].where(housing[&quot;income_cat&quot;] &lt; 5, 5.0, inplace&#x3D;True)</span><br><span class="line">from sklearn.model_selection import StratifiedShuffleSplit</span><br><span class="line">split &#x3D; StratifiedShuffleSplit(n_splits&#x3D;1, test_size&#x3D;0.2, random_state&#x3D;42)</span><br><span class="line">for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]):</span><br><span class="line">    strat_train_set &#x3D; housing.loc[train_index]</span><br><span class="line">    strat_test_set &#x3D; housing.loc[test_index]</span><br><span class="line">print(housing[&quot;income_cat&quot;].value_counts() &#x2F; len(housing))</span><br><span class="line">for set in (strat_train_set, strat_test_set):</span><br><span class="line">    set.drop([&quot;income_cat&quot;], axis&#x3D;1, inplace&#x3D;True)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>数据探索和可视化</p>
<ul>
<li>创建一个副本<code>housing = strat_train_set.copy()</code></li>
<li>将地理数据可视化<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#housing.plot(kind&#x3D;&quot;scatter&quot;, x&#x3D;&quot;longitude&quot;, y&#x3D;&quot;latitude&quot;)</span><br><span class="line">#housing.plot(kind&#x3D;&quot;scatter&quot;, x&#x3D;&quot;longitude&quot;, y&#x3D;&quot;latitude&quot;, alpha&#x3D;0.1)</span><br><span class="line">housing.plot(kind&#x3D;&quot;scatter&quot;, x&#x3D;&quot;longitude&quot;, y&#x3D;&quot;latitude&quot;, alpha&#x3D;0.4,</span><br><span class="line">s&#x3D;housing[&quot;population&quot;] &#x2F; 100, label&#x3D;&quot;population&quot;,</span><br><span class="line">c&#x3D;&quot;median_house_value&quot;, cmap&#x3D;plt.get_cmap(&quot;jet&quot;), colorbar&#x3D;True,)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li>寻找相关性<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#corr_matrix &#x3D; housing.corr()</span><br><span class="line">#print(corr_matrix[&quot;median_house_value&quot;].sort_values(ascending&#x3D;False))</span><br><span class="line">from pandas.plotting import scatter_matrix #少了tools</span><br><span class="line">attributes &#x3D; [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;]</span><br><span class="line">scatter_matrix(housing[attributes], figsize&#x3D;(12, 8))</span><br><span class="line">housing.plot(kind&#x3D;&quot;scatter&quot;, x&#x3D;&quot;median_income&quot;, y&#x3D;&quot;median_house_value&quot;, alpha&#x3D;0.1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>试验不同属性的组合</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">housing[&quot;rooms_per_household&quot;] &#x3D; housing[&quot;total_rooms&quot;]&#x2F;housing[&quot;households&quot;]</span><br><span class="line">housing[&quot;bedrooms_per_room&quot;] &#x3D; housing[&quot;total_bedrooms&quot;]&#x2F;housing[&quot;total_rooms&quot;]</span><br><span class="line">housing[&quot;population_per_household&quot;]&#x3D;housing[&quot;population&quot;]&#x2F;housing[&quot;households&quot;]</span><br><span class="line">corr_matrix &#x3D; housing.corr()</span><br><span class="line">print(corr_matrix[&quot;median_house_value&quot;].sort_values(ascending&#x3D;False))</span><br></pre></td></tr></table></figure>
</li>
<li><p>机器学习算法的数据准备</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">housing &#x3D; strat_train_set.drop(&quot;median_house_value&quot;, axis&#x3D;1)</span><br><span class="line">housing_labels &#x3D; strat_train_set[&quot;median_house_value&quot;].copy()</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据清理4选1</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#housing.dropna(subset&#x3D;[&quot;total_bedrooms&quot;])    # option 1</span><br><span class="line">#housing.drop(&quot;total_bedrooms&quot;, axis&#x3D;1)       # option 2</span><br><span class="line">#median &#x3D; housing[&quot;total_bedrooms&quot;].median()</span><br><span class="line">#housing[&quot;total_bedrooms&quot;].fillna(median)     # option 3</span><br><span class="line"></span><br><span class="line">#option4: Scikit-Learn提供的imputer, 指定你要用属性的中位数值替换该属性的缺失值</span><br><span class="line">from sklearn.impute import SimpleImputer #与书中不同，进化了</span><br><span class="line">imputer &#x3D; SimpleImputer(strategy&#x3D;&quot;median&quot;)   #创建一个imputer实例</span><br><span class="line">housing_num &#x3D; housing.drop(&quot;ocean_proximity&quot;, axis&#x3D;1)   #创建一个没有文本属性的数据副本ocean_proximity</span><br><span class="line">imputer.fit(housing_num)   #使用fit（）方法将imputer实例适配到训练集</span><br><span class="line">#print(imputer.statistics_)</span><br><span class="line">#print(housing_num.median().values)</span><br><span class="line">X &#x3D; imputer.transform(housing_num)   #替换</span><br><span class="line">housing_tr &#x3D; pd.DataFrame(X, columns&#x3D;housing_num.columns)   #放回Pandas DataFrame</span><br></pre></td></tr></table></figure>
</li>
<li><p>处理文本和分类属性</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#先将这些文本标签转化为数字，Scikit-Learn为这类任务提供了一个转换器LabelEncoder：</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line">encoder &#x3D; LabelEncoder()</span><br><span class="line">housing_cat &#x3D; housing[&quot;ocean_proximity&quot;]</span><br><span class="line">housing_cat_encoded &#x3D; encoder.fit_transform(housing_cat)</span><br><span class="line">#print(housing_cat_encoded)</span><br><span class="line">#print(encoder.classes_)</span><br><span class="line"></span><br><span class="line">#Scikit-Learn提供了一个OneHotEncoder编码器，可以将整数分类值转换为独热向量</span><br><span class="line">from sklearn.preprocessing import OneHotEncoder</span><br><span class="line">encoder &#x3D; OneHotEncoder()</span><br><span class="line">housing_cat_1hot &#x3D; encoder.fit_transform(housing_cat_encoded.reshape(-1,1))</span><br><span class="line">#print(housing_cat_1hot.toarray())</span><br><span class="line"></span><br><span class="line">#使用LabelBinarizer类可以一次性完成两个转换</span><br><span class="line">from sklearn.preprocessing import LabelBinarizer</span><br><span class="line">encoder &#x3D; LabelBinarizer()</span><br><span class="line">housing_cat_1hot &#x3D; encoder.fit_transform(housing_cat)</span><br><span class="line">print(housing_cat_1hot)</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义转换器</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line">rooms_ix, bedrooms_ix, population_ix, household_ix &#x3D; 3, 4, 5, 6</span><br><span class="line">class CombinedAttributesAdder(BaseEstimator, TransformerMixin):</span><br><span class="line">    def __init__(self, add_bedrooms_per_room &#x3D; True): # no *args or **kargs</span><br><span class="line">        self.add_bedrooms_per_room &#x3D; add_bedrooms_per_room</span><br><span class="line">    def fit(self, X, y&#x3D;None):</span><br><span class="line">        return self    #nothing else to do</span><br><span class="line">    def transform(self, X, y&#x3D;None):</span><br><span class="line">        rooms_per_household &#x3D; X[:, rooms_ix] &#x2F; X[:, household_ix]</span><br><span class="line">        population_per_household &#x3D; X[:, population_ix] &#x2F; X[:, household_ix]</span><br><span class="line">        if self.add_bedrooms_per_room:</span><br><span class="line">            bedrooms_per_room &#x3D; X[:, bedrooms_ix] &#x2F; X[:, rooms_ix]</span><br><span class="line">            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]</span><br><span class="line">        else:</span><br><span class="line">            return np.c_[X, rooms_per_household, population_per_household]</span><br><span class="line">attr_adder &#x3D; CombinedAttributesAdder(add_bedrooms_per_room&#x3D;False)</span><br><span class="line">housing_extra_attribs &#x3D; attr_adder.transform(housing.values)</span><br></pre></td></tr></table></figure>
</li>
<li><p>转换流水线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">num_pipeline &#x3D; Pipeline([</span><br><span class="line">    (&#39;imputer&#39;, SimpleImputer(strategy&#x3D;&quot;median&quot;)),</span><br><span class="line">    (&#39;attribs_adder&#39;, CombinedAttributesAdder()),</span><br><span class="line">    (&#39;std_scaler&#39;, StandardScaler()),</span><br><span class="line">    ])</span><br><span class="line">housing_num_tr &#x3D; num_pipeline.fit_transform(housing_num)</span><br><span class="line">#print(housing_num_tr)</span><br><span class="line"></span><br><span class="line">from sklearn.compose import ColumnTransformer</span><br><span class="line">num_attribs &#x3D; list(housing_num)</span><br><span class="line">cat_attribs &#x3D; [&quot;ocean_proximity&quot;]</span><br><span class="line"></span><br><span class="line">full_pipeline &#x3D; ColumnTransformer([</span><br><span class="line">    (&quot;num&quot;, num_pipeline, num_attribs),</span><br><span class="line">    (&quot;cat&quot;, OneHotEncoder(), cat_attribs),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">housing_prepared &#x3D; full_pipeline.fit_transform(housing)</span><br><span class="line">#print(housing_prepared)</span><br><span class="line">#print(housing_prepared.shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择和训练模型</p>
<ul>
<li>训练一个线性回归模型：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lin_reg &#x3D; LinearRegression()</span><br><span class="line">lin_reg.fit(housing_prepared, housing_labels)</span><br><span class="line">#print(lin_reg)</span><br><span class="line">#实例试试</span><br><span class="line">some_data &#x3D; housing.iloc[:5]</span><br><span class="line">some_labels &#x3D; housing_labels.iloc[:5]</span><br><span class="line">some_data_prepared &#x3D; full_pipeline.transform(some_data)</span><br><span class="line">#print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared))</span><br><span class="line">#print(&quot;Labels:&quot;, list(some_labels))</span><br><span class="line">#print(some_data_prepared)</span><br></pre></td></tr></table></figure></li>
<li>使用Scikit-Learn的mean_squared_error函数来测量整个训练集上回归模型的RMSE：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">housing_predictions &#x3D; lin_reg.predict(housing_prepared)</span><br><span class="line">lin_mse &#x3D; mean_squared_error(housing_labels, housing_predictions)</span><br><span class="line">lin_rmse &#x3D; np.sqrt(lin_mse)</span><br><span class="line">#print(lin_rmse)</span><br><span class="line">from sklearn.metrics import mean_absolute_error</span><br><span class="line">lin_mae &#x3D; mean_absolute_error(housing_labels, housing_predictions)</span><br><span class="line">#print(lin_mae)</span><br></pre></td></tr></table></figure></li>
<li>我们来训练一个(决策树)DecisionTreeRegressor。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">tree_reg &#x3D; DecisionTreeRegressor(random_state&#x3D;42)</span><br><span class="line">tree_reg.fit(housing_prepared, housing_labels)</span><br><span class="line">housing_predictions &#x3D; tree_reg.predict(housing_prepared)</span><br><span class="line">tree_mse &#x3D; mean_squared_error(housing_labels, housing_predictions)</span><br><span class="line">tree_rmse &#x3D; np.sqrt(tree_mse)</span><br><span class="line">#print(tree_rmse)    #可能对数据严重过度拟合</span><br></pre></td></tr></table></figure></li>
<li>使用交叉验证来更好地进行评估<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">scores &#x3D; cross_val_score(tree_reg, housing_prepared, housing_labels, scoring&#x3D;&quot;neg_mean_squared_error&quot;, cv&#x3D;10)</span><br><span class="line">tree_rmse_scores &#x3D; np.sqrt(-scores)</span><br><span class="line"></span><br><span class="line">def display_scores(scores):</span><br><span class="line">print(&quot;Scores:&quot;, scores)</span><br><span class="line">print(&quot;Mean:&quot;, scores.mean())</span><br><span class="line">print(&quot;Standard deviation:&quot;, scores.std())</span><br><span class="line">#display_scores(tree_rmse_scores)</span><br></pre></td></tr></table></figure></li>
<li>计算一下线性回归模型的评分<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lin_scores &#x3D; cross_val_score(lin_reg, housing_prepared, housing_labels, scoring&#x3D;&quot;neg_mean_squared_error&quot;, cv&#x3D;10)</span><br><span class="line">lin_rmse_scores &#x3D; np.sqrt(-lin_scores)</span><br><span class="line">#display_scores(lin_rmse_scores)</span><br></pre></td></tr></table></figure></li>
<li>随机森林模型RandomForestRegressor<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">forest_reg &#x3D; RandomForestRegressor(n_estimators&#x3D;10, random_state&#x3D;42)</span><br><span class="line">forest_reg.fit(housing_prepared, housing_labels)</span><br><span class="line">housing_predictions &#x3D; forest_reg.predict(housing_prepared)</span><br><span class="line">forest_mse &#x3D; mean_squared_error(housing_labels, housing_predictions)</span><br><span class="line">forest_rmse &#x3D; np.sqrt(forest_mse)</span><br><span class="line">#print(forest_rmse)</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">forest_scores &#x3D; cross_val_score(forest_reg, housing_prepared, housing_labels, scoring&#x3D;&quot;neg_mean_squared_error&quot;, cv&#x3D;10)</span><br><span class="line">forest_rmse_scores &#x3D; np.sqrt(-forest_scores)</span><br><span class="line">#display_scores(forest_rmse_scores)</span><br><span class="line">scores &#x3D; cross_val_score(lin_reg, housing_prepared, housing_labels, scoring&#x3D;&quot;neg_mean_squared_error&quot;, cv&#x3D;10)</span><br><span class="line">#print(pd.Series(np.sqrt(-scores)).describe())</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>微调模型</p>
</li>
<li><p>网格搜索</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#你可以用Scikit-Learn的GridSearchCV来替你进行探索。你所要做的只是告诉它你要进行实验的超参数是什么，以及需要尝试的值，它将会使用交叉验证来评估超参数值的所有可能的组合。</span><br><span class="line">#下面这段代码搜索RandomForestRegressor的超参数值的最佳组合:</span><br><span class="line">#当你不知道超参数应该赋什么值时，一个简单的方法是连续尝试10的幂次方</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">param_grid &#x3D; [</span><br><span class="line">    &#123;&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]&#125;, # try 12 (3×4) combinations of hyperparameters</span><br><span class="line">    &#123;&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]&#125;, # then try 6 (2×3) combinations with bootstrap set as False</span><br><span class="line">]</span><br><span class="line">forest_reg &#x3D; RandomForestRegressor()</span><br><span class="line">grid_search &#x3D; GridSearchCV(forest_reg, param_grid, cv&#x3D;5, scoring&#x3D;&#39;neg_mean_squared_error&#39;)</span><br><span class="line">grid_search.fit(housing_prepared, housing_labels)</span><br><span class="line">#print(grid_search.best_params_)</span><br><span class="line">#print(grid_search.best_estimator_)</span><br><span class="line"></span><br><span class="line">cvres &#x3D; grid_search.cv_results_</span><br><span class="line">for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]):</span><br><span class="line">   print(np.sqrt(-mean_score), params)</span><br><span class="line">print(pd.DataFrame(grid_search.cv_results_))</span><br><span class="line">#随机搜索</span><br><span class="line">#集成方法</span><br></pre></td></tr></table></figure>
</li>
<li><p>分析最佳模型及其错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">feature_importances &#x3D; grid_search.best_estimator_.feature_importances_</span><br><span class="line">#print(feature_importances)</span><br><span class="line">#将这些重要性分数显示在对应的属性名称旁边：</span><br><span class="line">extra_attribs &#x3D; [&quot;rooms_per_hhold&quot;, &quot;pop_per_hhold&quot;, &quot;bedrooms_per_room&quot;]</span><br><span class="line">#cat_encoder &#x3D; cat_pipeline.named_steps[&quot;cat_encoder&quot;] # old solution</span><br><span class="line">cat_encoder &#x3D; full_pipeline.named_transformers_[&quot;cat&quot;]</span><br><span class="line">cat_one_hot_attribs &#x3D; list(cat_encoder.categories_[0])</span><br><span class="line">attributes &#x3D; num_attribs + extra_attribs + cat_one_hot_attribs</span><br><span class="line">sorted(zip(feature_importances, attributes), reverse&#x3D;True)</span><br><span class="line">#print(sorted(zip(feature_importances, attributes), reverse&#x3D;True))</span><br><span class="line">#通过测试集评估系统</span><br><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">final_model &#x3D; grid_search.best_estimator_</span><br><span class="line">X_test &#x3D; strat_test_set.drop(&quot;median_house_value&quot;, axis&#x3D;1)</span><br><span class="line">y_test &#x3D; strat_test_set[&quot;median_house_value&quot;].copy()</span><br><span class="line">X_test_prepared &#x3D; full_pipeline.transform(X_test)</span><br><span class="line">final_predictions &#x3D; final_model.predict(X_test_prepared)</span><br><span class="line">final_mse &#x3D; mean_squared_error(y_test, final_predictions)</span><br><span class="line">final_rmse &#x3D; np.sqrt(final_mse)</span><br><span class="line">#print(final_rmse)</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动、监控和维护系统</p>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-c语言笔试准备" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/06/c%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AF%95%E5%87%86%E5%A4%87/" class="article-date">
      <time datetime="2020-03-06T12:23:02.000Z" itemprop="datePublished">2020-03-06</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/06/c%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AF%95%E5%87%86%E5%A4%87/">c语言笔试准备</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ol>
<li><p>若以下说明语句:char x; float y; double z; 则表达式x-y+z的类型为（double） 。<br>字节转换从低到高 char–&gt;float–&gt;short–&gt;int–&gt;double<br>规律：占用字节数小的类型在与占用字节数大的类型运算时会被转化为占用<strong>字节数大</strong>的类型。</p>
</li>
<li><p>设int a=3,b=5,m，执行表达式m=a&lt;=3&amp;&amp;a+b&lt;8后，m的值为 （0）<br><strong>！ &gt; 算术运算符 &gt; 关系运算符 &gt; &amp;&amp; &gt; || &gt; 赋值运算符</strong><br>m是整型，因此将false转为整型即为0。<br>m=1&amp;&amp;0<br>m=0</p>
</li>
<li><p>用数组名作函数实参时，形参可以用同类型的指针变量。<br>比如数组<br>int S[10];<br>数组名S可以理解为就是指向数组首地址的指针，即 S 和 &amp;S[0] 等价，*S和S[0]等价， 数组名作函数实参时，参数类型就是指向整型的指针</p>
</li>
<li><p>字符型数据在计算机内部是以ASCII码存储的，数字、英文大写字母和小写字母在ASCII码表中都是连续的。<strong>数字字符‘0’<del>‘9’是48</del>57</strong>，大写字母A～Z是从65～90，<strong>小写字母a～z是从97～122</strong>。</p>
</li>
<li><p>数组可以在<strong>定义</strong>时整体赋初值，但不能在赋值语句中整体赋值。</p>
</li>
<li><p>继承方式和可见性</p>
<ul>
<li>公有继承意味着继承派生类的类能访问基类的公有和保护成员。私有继承意味着继承派生类的类也不能访问基类的成员。保护继承意味着继承派生类的类能访问基类的公有和保护方法。</li>
<li>默认为私有继承，但常用的却是公有继承。</li>
<li>基类的私有成员在派生类中都是不可见的，如果一个派生类要访问基类中声明的私有成员，可以将这个派生类声明为友元。</li>
<li>公有继承时，同样继承了基类的私有成员，对基类的公有成员和保护成员的访问属性不变，派生类的新增成员可以访问基类的公有成员和保护成员，但是访问不了基类的私有成员。派生类的对象只能访问派生类的公有成员（包括继承的公有成员），访问不了保护成员和私有成员。</li>
</ul>
</li>
<li><p>可以把<code>z=x&gt;y? x : y</code>理解为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if(x&gt;y)&#123;</span><br><span class="line">z&#x3D;x；</span><br><span class="line">&#125;else&#123;</span><br><span class="line">z&#x3D;y；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于条件表达式b ? x : y，先计算条件b，然后进行判断。如果b的值为true，计算x的值，运算结果为x的值；否则，计算y的值，运算结果为y的值。一个条件表达式绝不会既计算x，又计算y。条件运算符是右结合的，也就是说，从右向左分组计算</p>
</li>
<li><p>auto被解释为一个自动存储变量的关键字，也就是申明一块临时的变量内存。<br>其中auto和register对应自动存储期。具有自动存储期的变量在进入声明该变量的程序块时被建立，它在该程序块活动时存在，退出该程序块时撤销。</p>
</li>
<li><p>register:这个关键字命令编译器尽可能的将变量存在CPU内部寄存器中而不是通过内存寻址访问以提高效率。如果一个变量被register来修饰，就意味着该变量作为一个寄存器变量，让该变量的访问速度达到最快。</p>
</li>
<li><p>全局变量在静态区;局部变量在动态区;static变量在静态区 </p>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c%E8%AF%AD%E8%A8%80/" rel="tag">c语言</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-c语言面试准备" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/05/c%E8%AF%AD%E8%A8%80%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/" class="article-date">
      <time datetime="2020-03-04T16:00:00.000Z" itemprop="datePublished">2020-03-05</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/05/c%E8%AF%AD%E8%A8%80%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/">c语言面试准备</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ol>
<li><p><strong>#define</strong>不能以分号结束，在宏中把参数用括号括起来</p>
<ul>
<li>写一个“标准”宏,这个宏输入两个参数并返回较小的：<code>#define MIN(x, y) ((x)&lt; (y)?(x):(y)) //结尾没有;</code></li>
<li>#是把宏参数转化为字符串的运算符，##是把两个宏参数连接的运算符。</li>
<li>为避免头文件my_ head.h被重复包含，可在其中使用条件编译:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#ifndef_ MY_ HEAD H</span><br><span class="line">#define_ MY_ HEAD_ H &#x2F;*空宏*&#x2F;</span><br><span class="line">&#x2F;*其他语句*&#x2F;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>预处理器直接计算常数表达式的值</p>
</li>
<li><p>死循环：while(1){}或for(;;){} </p>
<ul>
<li>两者相比for里面为空，编译执行之后没有判断的语句，而 while(1)始终都会有执行判断 1 = true，所以在单片机这种低速的、内存资源不多的环境，for(;;)是更好的选择。少执行判断语句，直接跳转（jump）到循环开始的代码继续执行。</li>
</ul>
</li>
<li><p>用变量a给出下面的定义 </p>
<ul>
<li>一个整型数: <code>int a</code></li>
<li>一个指向整型数的指针: <code>int *a</code></li>
<li>一个指向指针的指针，它指向的指针是指向一个整型数: <code>int **a</code></li>
<li>一个有10个整型数的数组: <code>int a[10]</code></li>
<li>一个有<u><strong>10个指针的数组，该指针是指向一个整型数的</strong></u>: <code>int *a[10]</code></li>
<li>一个指向有<u><strong>10个整型数数组的指针</strong></u>: <code>int (*a)[10]</code></li>
<li>一个指向<strong>函数</strong>的指针，该函数有一个整型参数并返回一个整型数： <code>int (*a)(int)</code></li>
<li>一个有10个指针的数组，该指针指向一个函数，该函数有一个整型参数并返回一个整型数： <code>int (*a[10])(int)</code></li>
</ul>
</li>
<li><p>关键字<strong>static</strong>的作用</p>
<ul>
<li>在函数体，一个被声明为静态的变量在这一函数被调用过程中维持其值不变。 </li>
<li>在模块内（但在函数体外），一个被声明为静态的<strong>变量</strong>可以被模块内所用函数<strong>访问</strong>，但不能被模块外其它函数访问。它是一个本地的全局变量。 </li>
<li>在模块内，一个被声明为静态的<strong>函数</strong>只可被这一模块内的其它函数<strong>调用</strong>。那就是，这个函数被限制在声明它的模块的本地范围内使用。</li>
</ul>
</li>
<li><p>关键字const有什么含意？</p>
<ul>
<li>它限定一个变量<strong>不允许被改变</strong>，产生静态作用。使用const在一定程度上可以提高程序的<strong>安全性和可靠性</strong>。另外，在观看别人代码的时候，清晰理解const所起的作用，对理解对方的程序也有一定帮助。</li>
<li>const 推出的初始目的，正是为了<strong>取代预编译指令，消除它的缺点，同时继承它的优点</strong>。</li>
<li>便于进行类型检查，使编译器对处理内容有更多了解，消除了一些隐患。</li>
<li>可以避免意义模糊的数字出现，同样可以很方便地进行参数的调整和修改。 同宏定义一样，可以做到不变则已，一变都变！</li>
<li>可以保护被修饰的东西，防止意外的修改，增强程序的健壮性,减少bug。</li>
<li>可以节省空间，避免不必要的内存分配。</li>
<li>意味着<strong>只读</strong>。<ul>
<li><code>const int a</code>； 或<code>int const a</code>； 表示a是一个常整型数。</li>
<li><code>const int *a</code>; 意味着a是一个指向常整型数的指针（<strong>整型数是不可修改的，但指针可以</strong></u>）。</li>
<li><code>int * const a</code>; a是一个指向整型数的常指针（<strong>指针指向的整型数是可以修改的，但指针是不可修改的</strong>）。</li>
<li><code>int const * a const</code>; 意味着a是一个指向常整型数的常指针（也就是说，<strong>指针指向的整型数是不可修改的，同时指针也是不可修改的</strong>）。</li>
<li><code>int fun(const int a);</code>或<code>int fun(const char *str);</code>修饰函数形参,使得形参在函数内不能被修改，表示输入参数。</li>
<li><code>const char *getstr(void);</code>使用: <code>const *str= getstr);</code><br><code>const int getint(void);</code>使用: <code>const int a =getint();</code>修饰函数返回值，使得函数的返回值不能被修改。</li>
</ul>
</li>
</ul>
</li>
<li><p>关键字volatile有什么含义？</p>
<ul>
<li>一个定义为volatile的变量是说<strong>这变量可能会被意想不到地改变</strong>，这样，编译器就不会去假设这个变量的值了。精确地说就是，优化器在用到这个变量时必须每次都小心地<strong>重新读取这个变量的值，而不是使用保存在寄存器里的备份</strong>。下面是volatile变量的几个例子：<ul>
<li>并行设备的硬件寄存器（如：状态寄存器） </li>
<li>一个中断服务子程序中会访问到的非自动变量(Non-automatic variables) </li>
<li>多线程应用中被几个任务共享的变量</li>
</ul>
</li>
<li>搞嵌入式的家伙们经常同硬件、中断、RTOS等等打交道，所有这些都要求用到volatile变量。</li>
<li>问题<ol>
<li>一个参数既可以是const还可以是volatile吗？解释为什么。<ul>
<li>一个例子是只读的状态寄存器。它是volatile因为它可能被意想不到地改变。它是const因为程序不应该试图去修改它。 </li>
</ul>
</li>
<li>一个指针可以是volatile 吗？解释为什么。 <ul>
<li>一个例子是当一个中服务子程序修该一个指向一个buffer的指针时。</li>
</ul>
</li>
</ol>
</li>
<li>volatile指定的关键字可能被系统、硬件、进程/线程改变,强制编译器每次从内存中取得该变量的值，而不是从被优化后的寄存器中读取。例子:硬件时钟;多线程中被多个任务共享的变量等。</li>
</ul>
</li>
<li><p>extern关键宇的作用：</p>
<ul>
<li>于修饰变量或函数，表明该变量或函数都是在别的文件中定义的,提示编译器在其他文件找定义。</li>
<li><code>extern &quot;C&quot;</code>的作用就是为了能够正确实现C+ +代码调其他C语言代码。</li>
</ul>
</li>
<li><p>要求设置一绝对地址为0x67a9的整型变量的值为0xaa66。编译器是一个纯粹的ANSI编译器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int *ptr; </span><br><span class="line">ptr &#x3D; (int *)0x67a9; </span><br><span class="line">*ptr &#x3D; 0xaa55;</span><br></pre></td></tr></table></figure>
</li>
<li><p>有关中断：</p>
<ul>
<li>ISR 不能返回一个值。如果你不懂这个，那么你不会被雇用。 </li>
<li>ISR 不能传递参数。如果你没有看到这一点，你被雇用的机会等同第一项。 </li>
<li>在许多的处理器/编译器中，浮点一般都是不可重入的。有些处理器/编译器需要让额处的寄存器入栈，有些处理器/编译器就是不允许在ISR中做浮点运算。此外，ISR应该是短而有效率的，在ISR中做浮点运算是不明智的。 </li>
<li>与第三点一脉相承，printf()经常有重入和性能上的问题。</li>
</ul>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c%E8%AF%AD%E8%A8%80/" rel="tag">c语言</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/03/hello-world/" class="article-date">
      <time datetime="2020-03-02T18:17:52.529Z" itemprop="datePublished">2020-03-03</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/03/hello-world/">Hello World</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2020 KissingIce
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 6;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>